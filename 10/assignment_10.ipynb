{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with non-linear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib import ticker, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of point in train data =  (500, 2)\n",
      "shape of point in test data =  (500, 2)\n",
      "shape of label in train data =  (500,)\n",
      "shape of label in test data =  (500,)\n",
      "data type of point x in train data =  float64\n",
      "data type of point y in train data =  float64\n",
      "data type of point x in test data =  float64\n",
      "data type of point y in test data =  float64\n"
     ]
    }
   ],
   "source": [
    "fname_data_train    = 'assignment_10_data_train.csv'\n",
    "fname_data_test     = 'assignment_10_data_test.csv'\n",
    "\n",
    "data_train          = np.genfromtxt(fname_data_train, delimiter=',')\n",
    "data_test           = np.genfromtxt(fname_data_test, delimiter=',')\n",
    "\n",
    "number_data_train   = data_train.shape[0]\n",
    "number_data_test    = data_test.shape[0]\n",
    "\n",
    "data_train_point    = data_train[:, 0:2]\n",
    "data_train_point_x  = data_train_point[:, 0]\n",
    "data_train_point_y  = data_train_point[:, 1]\n",
    "data_train_label    = data_train[:, 2]\n",
    "\n",
    "data_test_point     = data_test[:, 0:2]\n",
    "data_test_point_x   = data_test_point[:, 0]\n",
    "data_test_point_y   = data_test_point[:, 1]\n",
    "data_test_label     = data_test[:, 2]\n",
    "\n",
    "data_train_label_class_0    = (data_train_label == 0)\n",
    "data_train_label_class_1    = (data_train_label == 1)\n",
    "\n",
    "data_test_label_class_0     = (data_test_label == 0)\n",
    "data_test_label_class_1     = (data_test_label == 1)\n",
    "\n",
    "data_train_point_x_class_0  = data_train_point_x[data_train_label_class_0]\n",
    "data_train_point_y_class_0  = data_train_point_y[data_train_label_class_0]\n",
    "\n",
    "data_train_point_x_class_1  = data_train_point_x[data_train_label_class_1]\n",
    "data_train_point_y_class_1  = data_train_point_y[data_train_label_class_1]\n",
    "\n",
    "data_test_point_x_class_0   = data_test_point_x[data_test_label_class_0]\n",
    "data_test_point_y_class_0   = data_test_point_y[data_test_label_class_0]\n",
    "\n",
    "data_test_point_x_class_1   = data_test_point_x[data_test_label_class_1]\n",
    "data_test_point_y_class_1   = data_test_point_y[data_test_label_class_1]\n",
    "\n",
    "print('shape of point in train data = ', data_train_point.shape)\n",
    "print('shape of point in test data = ', data_train_point.shape)\n",
    "\n",
    "print('shape of label in train data = ', data_test_label.shape)\n",
    "print('shape of label in test data = ', data_test_label.shape)\n",
    "\n",
    "print('data type of point x in train data = ', data_train_point_x.dtype)\n",
    "print('data type of point y in train data = ', data_train_point_y.dtype)\n",
    "\n",
    "print('data type of point x in test data = ', data_test_point_x.dtype)\n",
    "print('data type of point y in test data = ', data_test_point_y.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(8,8))   \n",
    "\n",
    "plt.title('points in train data')\n",
    "plt.plot(data_train_point_x_class_0, data_train_point_y_class_0, 'o', color='blue', label='class = 0')\n",
    "plt.plot(data_train_point_x_class_1, data_train_point_y_class_1, 'o', color='red', label='class = 1')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(8,8))   \n",
    "\n",
    "plt.title('points in test data')\n",
    "plt.plot(data_test_point_x_class_0, data_test_point_y_class_0, 'o', color='blue', label='class = 0')\n",
    "plt.plot(data_test_point_x_class_1, data_test_point_y_class_1, 'o', color='red', label='class = 1')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the feature functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature vector is defined by $(1, f_1(x, y), f_2(x, y), \\cdots, f_{k-1}(x, y)) \\in \\mathbb{R}^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature(point):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "    n = np.shape(point)[0]\n",
    "    x = point[:,0]\n",
    "    y = point[:,1]\n",
    "\n",
    "    x_square = np.power(x,2)\n",
    "    x_cubic = np.power(x,3)\n",
    "    \n",
    "\n",
    "    feature = np.column_stack([np.ones(n), x, y, x_square, x_cubic])\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the linear regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta = (\\theta_0, \\theta_1, \\cdots, \\theta_{k-1}) \\in \\mathbb{R}^k$\n",
    "- feature = $(1, f_1(x, y), \\cdots, f_{k-1}(x, y)) \\in \\mathbb{R}^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_regression(theta, feature):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "    value = np.matmul(theta, feature.T)\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define sigmoid function with input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $z \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "    value = 1 / (1 + np.exp(-z))\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    return value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the logistic regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta = (\\theta_0, \\theta_1, \\cdots, \\theta_{k-1}) \\in \\mathbb{R}^k$\n",
    "- feature $= (1, f_1(x, y), \\cdots, f_{k-1}(x, y) \\in \\mathbb{R}^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_regression(theta, feature):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "    value = sigmoid(compute_linear_regression(theta, feature))\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the residual function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta = (\\theta_0, \\theta_1, \\cdots, \\theta_{k-1}) \\in \\mathbb{R}^k$\n",
    "- feature $= (1, f_1(x, y), \\cdots, f_{k-1}(x, y) \\in \\mathbb{R}^k$\n",
    "- label $= l \\in \\{0, 1\\}^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residual(theta, feature, label):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "    h = compute_logistic_regression(theta, feature)\n",
    "    residual = (-label * np.log(h)) - ((1-label) * np.log(1-h))\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the loss function for the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta = (\\theta_0, \\theta_1, \\cdots, \\theta_{k-1}) \\in \\mathbb{R}^k$\n",
    "- feature $= (1, f_1(x, y), \\cdots, f_{k-1}(x, y) \\in \\mathbb{R}^k$\n",
    "- label $= l \\in \\{0, 1\\}^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(theta, feature, label, alpha):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "    residual = compute_residual(theta, feature, label)\n",
    "    loss = (np.sum(residual) / np.shape(residual)[0]) + (np.inner(theta,theta)*alpha/2)\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the gradient of the loss with respect to the model parameter $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta = (\\theta_0, \\theta_1, \\cdots, \\theta_{k-1}) \\in \\mathbb{R}^k$\n",
    "- feature $= (1, f_1(x, y), \\cdots, f_{k-1}(x, y) \\in \\mathbb{R}^k$\n",
    "- label $= l \\in \\{0, 1\\}^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(theta, feature, label, alpha):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "    h = compute_logistic_regression(theta, feature)\n",
    "    gradient = (np.matmul(feature.T, (h-label)) / np.shape(label)[0]) + (alpha*theta)\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the accuracy of the prediction for point with a given model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(theta, feature, label):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "    n = np.shape(feature)[0]\n",
    "    pred = np.zeros(n)\n",
    "    h = compute_logistic_regression(theta, feature)\n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        if h[i] >= 0.5:\n",
    "            pred[i] = 1\n",
    "        if pred[i] == label[i]:\n",
    "                sum += 1\n",
    "\n",
    "    accuracy = sum / n\n",
    "\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_iteration    = 10000 # you can change this value as you want \n",
    "learning_rate       = 0.1 # you can change this value as you want \n",
    "number_feature      = 5 # you can change this value as you want\n",
    "alpha               = 0 # you can change this value as you want\n",
    "\n",
    "theta                       = np.zeros(number_feature)\n",
    "loss_iteration_train        = np.zeros(number_iteration)\n",
    "loss_iteration_test         = np.zeros(number_iteration)\n",
    "accuracy_iteration_train    = np.zeros(number_iteration)\n",
    "accuracy_iteration_test     = np.zeros(number_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the gradient descent algorithm to optimize the loss function with respect to the model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(number_iteration):\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    " \n",
    "    theta           = theta - learning_rate*compute_gradient(theta, compute_feature(data_train_point), data_train_label, alpha)\n",
    "    loss_train      = compute_loss(theta, compute_feature(data_train_point), data_train_label, alpha)\n",
    "    loss_test       = compute_loss(theta, compute_feature(data_test_point), data_test_label, alpha)\n",
    "    accuracy_train  = compute_accuracy(theta, compute_feature(data_train_point), data_train_label)\n",
    "    accuracy_test   = compute_accuracy(theta, compute_feature(data_test_point), data_test_label)\n",
    "\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    loss_iteration_train[i]     = loss_train\n",
    "    loss_iteration_test[i]      = loss_test\n",
    "    accuracy_iteration_train[i] = accuracy_train\n",
    "    accuracy_iteration_test[i]  = accuracy_test\n",
    "\n",
    "theta_optimal = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_iteration    = 40000 # you can change this value as you want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 18)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_alpha = np.linspace(0,1,101)\n",
    "ex_learning_rate = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "len(ex_alpha), len(ex_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_train        = []\n",
    "final_loss_test         = []\n",
    "final_accuracy_train    = []\n",
    "final_accuracy_test     = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nextgen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/home/nextgen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate =  1\n",
      "alpha =  0.0\n",
      "final training loss =  0.9180000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.9060000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.01\n",
      "final training loss =  0.8400000000\n",
      "final training accuracy =  0.4210565476\n",
      "final testing loss =  0.8200000000\n",
      "final testing accuracy =  0.4259344533\n",
      "learning_rate =  1\n",
      "alpha =  0.02\n",
      "final training loss =  0.8140000000\n",
      "final training accuracy =  0.5058824896\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.5053683167\n",
      "learning_rate =  1\n",
      "alpha =  0.03\n",
      "final training loss =  0.8060000000\n",
      "final training accuracy =  0.5309918341\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.5300596004\n",
      "learning_rate =  1\n",
      "alpha =  0.04\n",
      "final training loss =  0.8240000000\n",
      "final training accuracy =  0.4357045571\n",
      "final testing loss =  0.8380000000\n",
      "final testing accuracy =  0.4508590576\n",
      "learning_rate =  1\n",
      "alpha =  0.05\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4476441098\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4597747493\n",
      "learning_rate =  1\n",
      "alpha =  0.06\n",
      "final training loss =  0.8100000000\n",
      "final training accuracy =  0.4547100665\n",
      "final testing loss =  0.8140000000\n",
      "final testing accuracy =  0.4698997060\n",
      "learning_rate =  1\n",
      "alpha =  0.07\n",
      "final training loss =  0.8140000000\n",
      "final training accuracy =  0.4646239496\n",
      "final testing loss =  0.8160000000\n",
      "final testing accuracy =  0.4783583348\n",
      "learning_rate =  1\n",
      "alpha =  0.08\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5148728597\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.5199034503\n",
      "learning_rate =  1\n",
      "alpha =  0.09\n",
      "final training loss =  0.7820000000\n",
      "final training accuracy =  0.4749526540\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.4877928752\n",
      "learning_rate =  1\n",
      "alpha =  0.1\n",
      "final training loss =  0.7960000000\n",
      "final training accuracy =  0.4751841267\n",
      "final testing loss =  0.8020000000\n",
      "final testing accuracy =  0.4880478843\n",
      "learning_rate =  1\n",
      "alpha =  0.11\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.4914907630\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5028720520\n",
      "learning_rate =  1\n",
      "alpha =  0.12\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5091835923\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5172163057\n",
      "learning_rate =  1\n",
      "alpha =  0.13\n",
      "final training loss =  0.7200000000\n",
      "final training accuracy =  0.5748275225\n",
      "final testing loss =  0.7040000000\n",
      "final testing accuracy =  0.5844982081\n",
      "learning_rate =  1\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5250791195\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5387582467\n",
      "learning_rate =  1\n",
      "alpha =  0.15\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4975462093\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5098411099\n",
      "learning_rate =  1\n",
      "alpha =  0.16\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.17\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.6130626548\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.6137784351\n",
      "learning_rate =  1\n",
      "alpha =  0.18\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.5192073765\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5322501384\n",
      "learning_rate =  1\n",
      "alpha =  0.19\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  1.1241885376\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.2\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5529343050\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.5677979283\n",
      "learning_rate =  1\n",
      "alpha =  0.21\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5634058051\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5689111846\n",
      "learning_rate =  1\n",
      "alpha =  0.22\n",
      "final training loss =  0.7060000000\n",
      "final training accuracy =  0.6002551694\n",
      "final testing loss =  0.6800000000\n",
      "final testing accuracy =  0.6118852002\n",
      "learning_rate =  1\n",
      "alpha =  0.23\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  1.0011538597\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.9862160383\n",
      "learning_rate =  1\n",
      "alpha =  0.24\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5617172759\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.5763309479\n",
      "learning_rate =  1\n",
      "alpha =  0.25\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.6018233358\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.6073663915\n",
      "learning_rate =  1\n",
      "alpha =  0.26\n",
      "final training loss =  0.5980000000\n",
      "final training accuracy =  0.6612937981\n",
      "final testing loss =  0.6040000000\n",
      "final testing accuracy =  0.6679706743\n",
      "learning_rate =  1\n",
      "alpha =  0.27\n",
      "final training loss =  0.7300000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.28\n",
      "final training loss =  0.7260000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.29\n",
      "final training loss =  0.7240000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.3\n",
      "final training loss =  0.7220000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7100000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.31\n",
      "final training loss =  0.7220000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.32\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.7520633511\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.7498278505\n",
      "learning_rate =  1\n",
      "alpha =  0.33\n",
      "final training loss =  0.5020000000\n",
      "final training accuracy =  0.7378224019\n",
      "final testing loss =  0.5040000000\n",
      "final testing accuracy =  0.7392414882\n",
      "learning_rate =  1\n",
      "alpha =  0.34\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7552313259\n",
      "final testing loss =  0.5020000000\n",
      "final testing accuracy =  0.7557005129\n",
      "learning_rate =  1\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.36\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.7858097595\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.7826604969\n",
      "learning_rate =  1\n",
      "alpha =  0.37\n",
      "final training loss =  0.5020000000\n",
      "final training accuracy =  0.7645302660\n",
      "final testing loss =  0.5040000000\n",
      "final testing accuracy =  0.7656974679\n",
      "learning_rate =  1\n",
      "alpha =  0.38\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6372041987\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.6427845523\n",
      "learning_rate =  1\n",
      "alpha =  0.39\n",
      "final training loss =  0.7040000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7080000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.4\n",
      "final training loss =  0.7020000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7080000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.6941896602\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6967650641\n",
      "learning_rate =  1\n",
      "alpha =  0.42\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.7147808307\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.7163791593\n",
      "learning_rate =  1\n",
      "alpha =  0.43\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8049009411\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8041652616\n",
      "learning_rate =  1\n",
      "alpha =  0.44\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9093163746\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9024628488\n",
      "learning_rate =  1\n",
      "alpha =  0.45\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8983179607\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8922769562\n",
      "learning_rate =  1\n",
      "alpha =  0.46\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9193114293\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9122017393\n",
      "learning_rate =  1\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9419464748\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9336913036\n",
      "learning_rate =  1\n",
      "alpha =  0.48\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9662661293\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9567849868\n",
      "learning_rate =  1\n",
      "alpha =  0.49\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9922308879\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9814431571\n",
      "learning_rate =  1\n",
      "alpha =  0.5\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.0196855071\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.0075157383\n",
      "learning_rate =  1\n",
      "alpha =  0.51\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.0483280102\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.0347128923\n",
      "learning_rate =  1\n",
      "alpha =  0.52\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.0776876874\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.0625842807\n",
      "learning_rate =  1\n",
      "alpha =  0.53\n",
      "final training loss =  0.6840000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6760000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.54\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6740000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.55\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6720000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.56\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6680000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.8123964116\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.8113078065\n",
      "learning_rate =  1\n",
      "alpha =  0.58\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.2210504999\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.1983256221\n",
      "learning_rate =  1\n",
      "alpha =  0.59\n",
      "final training loss =  0.6700000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6660000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.6\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.2235894868\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.2003566288\n",
      "learning_rate =  1\n",
      "alpha =  0.61\n",
      "final training loss =  0.6660000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6640000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.62\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.1516775438\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.1313378990\n",
      "learning_rate =  1\n",
      "alpha =  0.63\n",
      "final training loss =  0.6800000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6640000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.64\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.6078926787\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6187425982\n",
      "learning_rate =  1\n",
      "alpha =  0.65\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.1294559136\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.1098598163\n",
      "learning_rate =  1\n",
      "alpha =  0.66\n",
      "final training loss =  0.6640000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6620000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  1\n",
      "alpha =  0.67\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.6858509345\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.6411154105\n",
      "learning_rate =  1\n",
      "alpha =  0.68\n",
      "final training loss =  0.7740000000\n",
      "final training accuracy =  0.6096817678\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.6145477782\n",
      "learning_rate =  1\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.7522876151\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.7043446337\n",
      "learning_rate =  1\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.2813989945\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.2575346188\n",
      "learning_rate =  1\n",
      "alpha =  0.71\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6306034990\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =  0.6343615218\n",
      "learning_rate =  1\n",
      "alpha =  0.72\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.7613734791\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.7124697837\n",
      "learning_rate =  1\n",
      "alpha =  0.73\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.8746709943\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.8214601855\n",
      "learning_rate =  1\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.6222755396\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6338531007\n",
      "learning_rate =  1\n",
      "alpha =  0.75\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6299099356\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6411993112\n",
      "learning_rate =  1\n",
      "alpha =  0.76\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6443955708\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.6549948456\n",
      "learning_rate =  1\n",
      "alpha =  0.77\n",
      "final training loss =  0.7180000000\n",
      "final training accuracy =  0.6690074094\n",
      "final testing loss =  0.7020000000\n",
      "final testing accuracy =  0.6782418233\n",
      "learning_rate =  1\n",
      "alpha =  0.78\n",
      "final training loss =  0.6460000000\n",
      "final training accuracy =  0.7085923821\n",
      "final testing loss =  0.6480000000\n",
      "final testing accuracy =  0.7154418585\n",
      "learning_rate =  1\n",
      "alpha =  0.79\n",
      "final training loss =  0.5240000000\n",
      "final training accuracy =  0.7757010777\n",
      "final testing loss =  0.5360000000\n",
      "final testing accuracy =  0.7785211799\n",
      "learning_rate =  1\n",
      "alpha =  0.8\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9588764132\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9521103696\n",
      "learning_rate =  1\n",
      "alpha =  0.81\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.1056859094\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.0920083351\n",
      "learning_rate =  1\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.1645336790\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.1482246180\n",
      "learning_rate =  1\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.2329330236\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.2135568102\n",
      "learning_rate =  1\n",
      "alpha =  0.84\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.3113967604\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.2884951277\n",
      "learning_rate =  1\n",
      "alpha =  0.85\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.3998698746\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.3729910292\n",
      "learning_rate =  1\n",
      "alpha =  0.86\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.4977654717\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.4664916192\n",
      "learning_rate =  1\n",
      "alpha =  0.87\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.6041443280\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.5681101768\n",
      "learning_rate =  1\n",
      "alpha =  0.88\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.7179375667\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.6768374541\n",
      "learning_rate =  1\n",
      "alpha =  0.89\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.8381210718\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.7917070701\n",
      "learning_rate =  1\n",
      "alpha =  0.9\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.9638104491\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.9118854530\n",
      "learning_rate =  1\n",
      "alpha =  0.91\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  2.0942913630\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  2.0367004440\n",
      "learning_rate =  1\n",
      "alpha =  0.92\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  2.2290128460\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  2.1656347840\n",
      "learning_rate =  1\n",
      "alpha =  0.93\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  2.3675652099\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  2.2983050340\n",
      "learning_rate =  1\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  2.5096547578\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  2.4344375144\n",
      "learning_rate =  1\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  2.6550806926\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  2.5738463812\n",
      "learning_rate =  1\n",
      "alpha =  0.96\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  2.8037159353\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  2.7164154527\n",
      "learning_rate =  1\n",
      "alpha =  0.97\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  2.9554919131\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  2.8620838304\n",
      "learning_rate =  1\n",
      "alpha =  0.98\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  3.1103867849\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  3.0108347964\n",
      "learning_rate =  1\n",
      "alpha =  0.99\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  3.2684164438\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  3.1626873532\n",
      "learning_rate =  1\n",
      "alpha =  1.0\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  3.4296276960\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  3.3176898267\n",
      "learning_rate =  0.9\n",
      "alpha =  0.0\n",
      "final training loss =  0.9180000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.9060000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.01\n",
      "final training loss =  0.8480000000\n",
      "final training accuracy =  0.3992575948\n",
      "final testing loss =  0.8200000000\n",
      "final testing accuracy =  0.4067470679\n",
      "learning_rate =  0.9\n",
      "alpha =  0.02\n",
      "final training loss =  0.8180000000\n",
      "final training accuracy =  0.4437496222\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4491262768\n",
      "learning_rate =  0.9\n",
      "alpha =  0.03\n",
      "final training loss =  0.8040000000\n",
      "final training accuracy =  0.4709070131\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4752956459\n",
      "learning_rate =  0.9\n",
      "alpha =  0.04\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4687006360\n",
      "final testing loss =  0.8080000000\n",
      "final testing accuracy =  0.4748085547\n",
      "learning_rate =  0.9\n",
      "alpha =  0.05\n",
      "final training loss =  0.7900000000\n",
      "final training accuracy =  0.5407228166\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5410972653\n",
      "learning_rate =  0.9\n",
      "alpha =  0.06\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.5640789256\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5634461484\n",
      "learning_rate =  0.9\n",
      "alpha =  0.07\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4779955766\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4862060015\n",
      "learning_rate =  0.9\n",
      "alpha =  0.08\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5004489353\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.5067129484\n",
      "learning_rate =  0.9\n",
      "alpha =  0.09\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5516006537\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5537686384\n",
      "learning_rate =  0.9\n",
      "alpha =  0.1\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.4950161917\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5030699666\n",
      "learning_rate =  0.9\n",
      "alpha =  0.11\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.7572038765\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.7479122539\n",
      "learning_rate =  0.9\n",
      "alpha =  0.12\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.7042043882\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.6978942228\n",
      "learning_rate =  0.9\n",
      "alpha =  0.13\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5944395427\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5950711414\n",
      "learning_rate =  0.9\n",
      "alpha =  0.14\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.5133888754\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5264549541\n",
      "learning_rate =  0.9\n",
      "alpha =  0.15\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.8523955375\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.8404462249\n",
      "learning_rate =  0.9\n",
      "alpha =  0.16\n",
      "final training loss =  0.7820000000\n",
      "final training accuracy =  0.5027426933\n",
      "final testing loss =  0.7740000000\n",
      "final testing accuracy =  0.5157476686\n",
      "learning_rate =  0.9\n",
      "alpha =  0.17\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.6001375363\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.6014574399\n",
      "learning_rate =  0.9\n",
      "alpha =  0.18\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6652863223\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6629433847\n",
      "learning_rate =  0.9\n",
      "alpha =  0.19\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.6824834455\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.6794909905\n",
      "learning_rate =  0.9\n",
      "alpha =  0.2\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  1.0801232877\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.21\n",
      "final training loss =  0.7820000000\n",
      "final training accuracy =  0.5324841763\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5442757645\n",
      "learning_rate =  0.9\n",
      "alpha =  0.22\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.7694991240\n",
      "final testing loss =  0.7340000000\n",
      "final testing accuracy =  0.7629614370\n",
      "learning_rate =  0.9\n",
      "alpha =  0.23\n",
      "final training loss =  0.6000000000\n",
      "final training accuracy =  0.6448648382\n",
      "final testing loss =  0.6100000000\n",
      "final testing accuracy =  0.6487677494\n",
      "learning_rate =  0.9\n",
      "alpha =  0.24\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5511006968\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5580266834\n",
      "learning_rate =  0.9\n",
      "alpha =  0.25\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  1.0059479182\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.9914611022\n",
      "learning_rate =  0.9\n",
      "alpha =  0.26\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5696047971\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.5822771577\n",
      "learning_rate =  0.9\n",
      "alpha =  0.27\n",
      "final training loss =  0.7260000000\n",
      "final training accuracy =  0.5948511952\n",
      "final testing loss =  0.7060000000\n",
      "final testing accuracy =  0.6057734505\n",
      "learning_rate =  0.9\n",
      "alpha =  0.28\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5682949926\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.5784102013\n",
      "learning_rate =  0.9\n",
      "alpha =  0.29\n",
      "final training loss =  0.7300000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.3\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5515690626\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5627126462\n",
      "learning_rate =  0.9\n",
      "alpha =  0.31\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5360072386\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5469066741\n",
      "learning_rate =  0.9\n",
      "alpha =  0.32\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.7980880069\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.7921926701\n",
      "learning_rate =  0.9\n",
      "alpha =  0.33\n",
      "final training loss =  0.7300000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.34\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7206608428\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7211376793\n",
      "learning_rate =  0.9\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.6040000000\n",
      "final training accuracy =  0.6550465365\n",
      "final testing loss =  0.6180000000\n",
      "final testing accuracy =  0.6602862598\n",
      "learning_rate =  0.9\n",
      "alpha =  0.36\n",
      "final training loss =  0.5580000000\n",
      "final training accuracy =  0.6895571643\n",
      "final testing loss =  0.5640000000\n",
      "final testing accuracy =  0.6933036452\n",
      "learning_rate =  0.9\n",
      "alpha =  0.37\n",
      "final training loss =  0.5300000000\n",
      "final training accuracy =  0.7014640302\n",
      "final testing loss =  0.5460000000\n",
      "final testing accuracy =  0.7046395844\n",
      "learning_rate =  0.9\n",
      "alpha =  0.38\n",
      "final training loss =  0.7220000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.39\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6755097546\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.6781965812\n",
      "learning_rate =  0.9\n",
      "alpha =  0.4\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.6871185833\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6893057229\n",
      "learning_rate =  0.9\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6982568234\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6999855880\n",
      "learning_rate =  0.9\n",
      "alpha =  0.42\n",
      "final training loss =  0.7160000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.43\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.7193875993\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.7202996494\n",
      "learning_rate =  0.9\n",
      "alpha =  0.44\n",
      "final training loss =  0.7140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.45\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.7390208494\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.7392349972\n",
      "learning_rate =  0.9\n",
      "alpha =  0.46\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.7481925409\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.7481019930\n",
      "learning_rate =  0.9\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.7568510516\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.7564869869\n",
      "learning_rate =  0.9\n",
      "alpha =  0.48\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8247971145\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8214954830\n",
      "learning_rate =  0.9\n",
      "alpha =  0.49\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8387631698\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8347416592\n",
      "learning_rate =  0.9\n",
      "alpha =  0.5\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8534534824\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8486792013\n",
      "learning_rate =  0.9\n",
      "alpha =  0.51\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8688971127\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8633354319\n",
      "learning_rate =  0.9\n",
      "alpha =  0.52\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8851007048\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8787160999\n",
      "learning_rate =  0.9\n",
      "alpha =  0.53\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9020387487\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8947961070\n",
      "learning_rate =  0.9\n",
      "alpha =  0.54\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9196417373\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9115082476\n",
      "learning_rate =  0.9\n",
      "alpha =  0.55\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9377826340\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9287303568\n",
      "learning_rate =  0.9\n",
      "alpha =  0.56\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9562623683\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9462715481\n",
      "learning_rate =  0.9\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9747951315\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9638582696\n",
      "learning_rate =  0.9\n",
      "alpha =  0.58\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9929937083\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9811203866\n",
      "learning_rate =  0.9\n",
      "alpha =  0.59\n",
      "final training loss =  0.6920000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6900000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.6\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.7714718943\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.7719584657\n",
      "learning_rate =  0.9\n",
      "alpha =  0.61\n",
      "final training loss =  0.6860000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6880000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.62\n",
      "final training loss =  0.6840000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6800000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.63\n",
      "final training loss =  0.6860000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6780000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.64\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6780000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.65\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.7001215058\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.7044253429\n",
      "learning_rate =  0.9\n",
      "alpha =  0.66\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6707857530\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.6766940309\n",
      "learning_rate =  0.9\n",
      "alpha =  0.67\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6680000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.68\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.0520619576\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.0363445865\n",
      "learning_rate =  0.9\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6740000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.6800000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6660000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.71\n",
      "final training loss =  0.7260000000\n",
      "final training accuracy =  0.6373302508\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =  0.6458308154\n",
      "learning_rate =  0.9\n",
      "alpha =  0.72\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6573984221\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6607096193\n",
      "learning_rate =  0.9\n",
      "alpha =  0.73\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9511505962\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9396702622\n",
      "learning_rate =  0.9\n",
      "alpha =  0.74\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.6112757783\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.6217728840\n",
      "learning_rate =  0.9\n",
      "alpha =  0.75\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.5041306568\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.4672328190\n",
      "learning_rate =  0.9\n",
      "alpha =  0.76\n",
      "final training loss =  0.6960000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7020000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.77\n",
      "final training loss =  0.7140000000\n",
      "final training accuracy =  1.5294901207\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =  1.5115032182\n",
      "learning_rate =  0.9\n",
      "alpha =  0.78\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.6169216170\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.6215927283\n",
      "learning_rate =  0.9\n",
      "alpha =  0.79\n",
      "final training loss =  0.6440000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6360000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.9\n",
      "alpha =  0.8\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9855463472\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9754044517\n",
      "learning_rate =  0.9\n",
      "alpha =  0.81\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.6246594194\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6350219134\n",
      "learning_rate =  0.9\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.6188384363\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6294864007\n",
      "learning_rate =  0.9\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6257271201\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6360555473\n",
      "learning_rate =  0.9\n",
      "alpha =  0.84\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6372550308\n",
      "final testing loss =  0.7200000000\n",
      "final testing accuracy =  0.6469778852\n",
      "learning_rate =  0.9\n",
      "alpha =  0.85\n",
      "final training loss =  0.7240000000\n",
      "final training accuracy =  0.6555629208\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =  0.6642358689\n",
      "learning_rate =  0.9\n",
      "alpha =  0.86\n",
      "final training loss =  0.6900000000\n",
      "final training accuracy =  0.6835140570\n",
      "final testing loss =  0.6680000000\n",
      "final testing accuracy =  0.6904964771\n",
      "learning_rate =  0.9\n",
      "alpha =  0.87\n",
      "final training loss =  0.5900000000\n",
      "final training accuracy =  0.7268224522\n",
      "final testing loss =  0.5840000000\n",
      "final testing accuracy =  0.7311691730\n",
      "learning_rate =  0.9\n",
      "alpha =  0.88\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8035953483\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8035675766\n",
      "learning_rate =  0.9\n",
      "alpha =  0.89\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.0247806657\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.0137983088\n",
      "learning_rate =  0.9\n",
      "alpha =  0.9\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.0648947347\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.0521233440\n",
      "learning_rate =  0.9\n",
      "alpha =  0.91\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.1106881984\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.0958699000\n",
      "learning_rate =  0.9\n",
      "alpha =  0.92\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.1627332488\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.1455831447\n",
      "learning_rate =  0.9\n",
      "alpha =  0.93\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.2214291063\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.2016440591\n",
      "learning_rate =  0.9\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.2869340407\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.2642050596\n",
      "learning_rate =  0.9\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.3591420852\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.3331680287\n",
      "learning_rate =  0.9\n",
      "alpha =  0.96\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.4377151860\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.4082149012\n",
      "learning_rate =  0.9\n",
      "alpha =  0.97\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.5221565807\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.4888773077\n",
      "learning_rate =  0.9\n",
      "alpha =  0.98\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.6118965017\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.5746178657\n",
      "learning_rate =  0.9\n",
      "alpha =  0.99\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.7063647869\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.6648990239\n",
      "learning_rate =  0.9\n",
      "alpha =  1.0\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.8050387761\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.7592284482\n",
      "learning_rate =  0.8\n",
      "alpha =  0.0\n",
      "final training loss =  0.9180000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.9040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.8\n",
      "alpha =  0.01\n",
      "final training loss =  0.8580000000\n",
      "final training accuracy =  0.3723293517\n",
      "final testing loss =  0.8520000000\n",
      "final testing accuracy =  0.3906701519\n",
      "learning_rate =  0.8\n",
      "alpha =  0.02\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4212258804\n",
      "final testing loss =  0.8220000000\n",
      "final testing accuracy =  0.4292148196\n",
      "learning_rate =  0.8\n",
      "alpha =  0.03\n",
      "final training loss =  0.8100000000\n",
      "final training accuracy =  0.4455069745\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4525409227\n",
      "learning_rate =  0.8\n",
      "alpha =  0.04\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4635860068\n",
      "final testing loss =  0.8080000000\n",
      "final testing accuracy =  0.4700003368\n",
      "learning_rate =  0.8\n",
      "alpha =  0.05\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4785357083\n",
      "final testing loss =  0.8060000000\n",
      "final testing accuracy =  0.4844519268\n",
      "learning_rate =  0.8\n",
      "alpha =  0.06\n",
      "final training loss =  0.7800000000\n",
      "final training accuracy =  0.4916653084\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4971355883\n",
      "learning_rate =  0.8\n",
      "alpha =  0.07\n",
      "final training loss =  0.7740000000\n",
      "final training accuracy =  0.5036389840\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5086885518\n",
      "learning_rate =  0.8\n",
      "alpha =  0.08\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5148344065\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5194769278\n",
      "learning_rate =  0.8\n",
      "alpha =  0.09\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5254848336\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5297287157\n",
      "learning_rate =  0.8\n",
      "alpha =  0.1\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5357438735\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5395950409\n",
      "learning_rate =  0.8\n",
      "alpha =  0.11\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5287647746\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5336945527\n",
      "learning_rate =  0.8\n",
      "alpha =  0.12\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5253135881\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5309840042\n",
      "learning_rate =  0.8\n",
      "alpha =  0.13\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5247188682\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5308349457\n",
      "learning_rate =  0.8\n",
      "alpha =  0.14\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5251038640\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5315421243\n",
      "learning_rate =  0.8\n",
      "alpha =  0.15\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.6393402264\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.6376414557\n",
      "learning_rate =  0.8\n",
      "alpha =  0.16\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.6590132077\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.6564540411\n",
      "learning_rate =  0.8\n",
      "alpha =  0.17\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.6591815141\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.6568753891\n",
      "learning_rate =  0.8\n",
      "alpha =  0.18\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.7021854339\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.6978330603\n",
      "learning_rate =  0.8\n",
      "alpha =  0.19\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5614576101\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5661049519\n",
      "learning_rate =  0.8\n",
      "alpha =  0.2\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5099862959\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5215029475\n",
      "learning_rate =  0.8\n",
      "alpha =  0.21\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.7688442593\n",
      "final testing loss =  0.7340000000\n",
      "final testing accuracy =  0.7620462490\n",
      "learning_rate =  0.8\n",
      "alpha =  0.22\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5137261891\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5249952388\n",
      "learning_rate =  0.8\n",
      "alpha =  0.23\n",
      "final training loss =  0.7800000000\n",
      "final training accuracy =  0.5321942822\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5434879361\n",
      "learning_rate =  0.8\n",
      "alpha =  0.24\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5261191988\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5350555859\n",
      "learning_rate =  0.8\n",
      "alpha =  0.25\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5742365532\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5790347946\n",
      "learning_rate =  0.8\n",
      "alpha =  0.26\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.9682789171\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.9551366622\n",
      "learning_rate =  0.8\n",
      "alpha =  0.27\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.5323925230\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5427342278\n",
      "learning_rate =  0.8\n",
      "alpha =  0.28\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5377669720\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5473080493\n",
      "learning_rate =  0.8\n",
      "alpha =  0.29\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5762357376\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.5853400008\n",
      "learning_rate =  0.8\n",
      "alpha =  0.3\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.8280181153\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.8206027782\n",
      "learning_rate =  0.8\n",
      "alpha =  0.31\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.5639587547\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5720148197\n",
      "learning_rate =  0.8\n",
      "alpha =  0.32\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  1.1269377917\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  1.1104888293\n",
      "learning_rate =  0.8\n",
      "alpha =  0.33\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5310581286\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5406037230\n",
      "learning_rate =  0.8\n",
      "alpha =  0.34\n",
      "final training loss =  0.6240000000\n",
      "final training accuracy =  0.6437255049\n",
      "final testing loss =  0.6240000000\n",
      "final testing accuracy =  0.6477812061\n",
      "learning_rate =  0.8\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5790574989\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5867409688\n",
      "learning_rate =  0.8\n",
      "alpha =  0.36\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6264305275\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.6307595070\n",
      "learning_rate =  0.8\n",
      "alpha =  0.37\n",
      "final training loss =  0.6760000000\n",
      "final training accuracy =  0.6317785755\n",
      "final testing loss =  0.6640000000\n",
      "final testing accuracy =  0.6386616333\n",
      "learning_rate =  0.8\n",
      "alpha =  0.38\n",
      "final training loss =  0.7220000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7100000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.8\n",
      "alpha =  0.39\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.6654026232\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6678641454\n",
      "learning_rate =  0.8\n",
      "alpha =  0.4\n",
      "final training loss =  0.7220000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.8\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.5567608215\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.5670637241\n",
      "learning_rate =  0.8\n",
      "alpha =  0.42\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7291073052\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7282916011\n",
      "learning_rate =  0.8\n",
      "alpha =  0.43\n",
      "final training loss =  0.6860000000\n",
      "final training accuracy =  0.6275684652\n",
      "final testing loss =  0.6740000000\n",
      "final testing accuracy =  0.6335846503\n",
      "learning_rate =  0.8\n",
      "alpha =  0.44\n",
      "final training loss =  0.7220000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.8\n",
      "alpha =  0.45\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5967403962\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6050422051\n",
      "learning_rate =  0.8\n",
      "alpha =  0.46\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5534106847\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5629343666\n",
      "learning_rate =  0.8\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  1.0452268394\n",
      "final testing loss =  0.7220000000\n",
      "final testing accuracy =  1.0334121530\n",
      "learning_rate =  0.8\n",
      "alpha =  0.48\n",
      "final training loss =  0.5560000000\n",
      "final training accuracy =  0.6826858503\n",
      "final testing loss =  0.5620000000\n",
      "final testing accuracy =  0.6853778565\n",
      "learning_rate =  0.8\n",
      "alpha =  0.49\n",
      "final training loss =  0.7200000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.8\n",
      "alpha =  0.5\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8218864867\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8161129658\n",
      "learning_rate =  0.8\n",
      "alpha =  0.51\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5735973258\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5819811814\n",
      "learning_rate =  0.8\n",
      "alpha =  0.52\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.7636299395\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.7629626103\n",
      "learning_rate =  0.8\n",
      "alpha =  0.53\n",
      "final training loss =  0.5700000000\n",
      "final training accuracy =  0.6775969307\n",
      "final testing loss =  0.5680000000\n",
      "final testing accuracy =  0.6803526563\n",
      "learning_rate =  0.8\n",
      "alpha =  0.54\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7443431531\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7438034443\n",
      "learning_rate =  0.8\n",
      "alpha =  0.55\n",
      "final training loss =  0.7200000000\n",
      "final training accuracy =  1.5546480983\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.8\n",
      "alpha =  0.56\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6442812185\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.6499435250\n",
      "learning_rate =  0.8\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7661981328\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7644559222\n",
      "learning_rate =  0.8\n",
      "alpha =  0.58\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7672091440\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7653290766\n",
      "learning_rate =  0.8\n",
      "alpha =  0.59\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7955941473\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7924222430\n",
      "learning_rate =  0.8\n",
      "alpha =  0.6\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8053970633\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8017178886\n",
      "learning_rate =  0.8\n",
      "alpha =  0.61\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8113805829\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8073531038\n",
      "learning_rate =  0.8\n",
      "alpha =  0.62\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6319177924\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.6387248779\n",
      "learning_rate =  0.8\n",
      "alpha =  0.63\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6355632856\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.6422504655\n",
      "learning_rate =  0.8\n",
      "alpha =  0.64\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.7597341207\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.7584297958\n",
      "learning_rate =  0.8\n",
      "alpha =  0.65\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8945323177\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8868592526\n",
      "learning_rate =  0.8\n",
      "alpha =  0.66\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5984581944\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.6074637818\n",
      "learning_rate =  0.8\n",
      "alpha =  0.67\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8637363543\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8572394197\n",
      "learning_rate =  0.8\n",
      "alpha =  0.68\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6357625246\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.6428192589\n",
      "learning_rate =  0.8\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9374217626\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9274689160\n",
      "learning_rate =  0.8\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8872786116\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8793459433\n",
      "learning_rate =  0.8\n",
      "alpha =  0.71\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7785081802\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7767045012\n",
      "learning_rate =  0.8\n",
      "alpha =  0.72\n",
      "final training loss =  0.7300000000\n",
      "final training accuracy =  1.0071995718\n",
      "final testing loss =  0.7180000000\n",
      "final testing accuracy =  0.9989657413\n",
      "learning_rate =  0.8\n",
      "alpha =  0.73\n",
      "final training loss =  0.7300000000\n",
      "final training accuracy =  0.9946518510\n",
      "final testing loss =  0.7180000000\n",
      "final testing accuracy =  0.9868250097\n",
      "learning_rate =  0.8\n",
      "alpha =  0.74\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7515426363\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7495908070\n",
      "learning_rate =  0.8\n",
      "alpha =  0.75\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6658471641\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6717711250\n",
      "learning_rate =  0.8\n",
      "alpha =  0.76\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6659965045\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6719820318\n",
      "learning_rate =  0.8\n",
      "alpha =  0.77\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8551959950\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8483395772\n",
      "learning_rate =  0.8\n",
      "alpha =  0.78\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.6159098165\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6210387190\n",
      "learning_rate =  0.8\n",
      "alpha =  0.79\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5736376311\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5812711367\n",
      "learning_rate =  0.8\n",
      "alpha =  0.8\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5950739529\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6053197585\n",
      "learning_rate =  0.8\n",
      "alpha =  0.81\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =  0.6628634132\n",
      "final testing loss =  0.6680000000\n",
      "final testing accuracy =  0.6686761008\n",
      "learning_rate =  0.8\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7320000000\n",
      "final training accuracy =  0.9083133848\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =  0.9034865210\n",
      "learning_rate =  0.8\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.7168801915\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.7182569099\n",
      "learning_rate =  0.8\n",
      "alpha =  0.84\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7008316596\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7010105196\n",
      "learning_rate =  0.8\n",
      "alpha =  0.85\n",
      "final training loss =  0.6700000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6680000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.8\n",
      "alpha =  0.86\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.6155303799\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.6203841458\n",
      "learning_rate =  0.8\n",
      "alpha =  0.87\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.6050955627\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.6150632719\n",
      "learning_rate =  0.8\n",
      "alpha =  0.88\n",
      "final training loss =  0.5060000000\n",
      "final training accuracy =  0.6832209802\n",
      "final testing loss =  0.5180000000\n",
      "final testing accuracy =  0.6838325889\n",
      "learning_rate =  0.8\n",
      "alpha =  0.89\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.2866960745\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.2594331241\n",
      "learning_rate =  0.8\n",
      "alpha =  0.9\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.3647744476\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  1.3340479219\n",
      "learning_rate =  0.8\n",
      "alpha =  0.91\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6083854840\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6184926812\n",
      "learning_rate =  0.8\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6180209472\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6276222209\n",
      "learning_rate =  0.8\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6244360631\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6336926732\n",
      "learning_rate =  0.8\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6340056946\n",
      "final testing loss =  0.7200000000\n",
      "final testing accuracy =  0.6427143538\n",
      "learning_rate =  0.8\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.6480456964\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =  0.6559144864\n",
      "learning_rate =  0.8\n",
      "alpha =  0.96\n",
      "final training loss =  0.7080000000\n",
      "final training accuracy =  0.6683043028\n",
      "final testing loss =  0.6820000000\n",
      "final testing accuracy =  0.6749306641\n",
      "learning_rate =  0.8\n",
      "alpha =  0.97\n",
      "final training loss =  0.6320000000\n",
      "final training accuracy =  0.6977403289\n",
      "final testing loss =  0.6300000000\n",
      "final testing accuracy =  0.7025624074\n",
      "learning_rate =  0.8\n",
      "alpha =  0.98\n",
      "final training loss =  0.5280000000\n",
      "final training accuracy =  0.7431711004\n",
      "final testing loss =  0.5400000000\n",
      "final testing accuracy =  0.7453154575\n",
      "learning_rate =  0.8\n",
      "alpha =  0.99\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8325508504\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8299061164\n",
      "learning_rate =  0.8\n",
      "alpha =  1.0\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.9848623923\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9748345933\n",
      "learning_rate =  0.7\n",
      "alpha =  0.0\n",
      "final training loss =  0.9140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.9040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.01\n",
      "final training loss =  0.8640000000\n",
      "final training accuracy =  0.3659922136\n",
      "final testing loss =  0.8580000000\n",
      "final testing accuracy =  0.3823846664\n",
      "learning_rate =  0.7\n",
      "alpha =  0.02\n",
      "final training loss =  0.8420000000\n",
      "final training accuracy =  0.3996398142\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.4148224626\n",
      "learning_rate =  0.7\n",
      "alpha =  0.03\n",
      "final training loss =  0.8160000000\n",
      "final training accuracy =  0.4248916973\n",
      "final testing loss =  0.8160000000\n",
      "final testing accuracy =  0.4346274158\n",
      "learning_rate =  0.7\n",
      "alpha =  0.04\n",
      "final training loss =  0.8020000000\n",
      "final training accuracy =  0.4408117148\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4499409014\n",
      "learning_rate =  0.7\n",
      "alpha =  0.05\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4537095483\n",
      "final testing loss =  0.8080000000\n",
      "final testing accuracy =  0.4623825720\n",
      "learning_rate =  0.7\n",
      "alpha =  0.06\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4648205452\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4731013668\n",
      "learning_rate =  0.7\n",
      "alpha =  0.07\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4747789876\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4826983460\n",
      "learning_rate =  0.7\n",
      "alpha =  0.08\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.4839488678\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4915231978\n",
      "learning_rate =  0.7\n",
      "alpha =  0.09\n",
      "final training loss =  0.7740000000\n",
      "final training accuracy =  0.4925562583\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4997953795\n",
      "learning_rate =  0.7\n",
      "alpha =  0.1\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5007502512\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.5076607633\n",
      "learning_rate =  0.7\n",
      "alpha =  0.11\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5086339901\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5152208564\n",
      "learning_rate =  0.7\n",
      "alpha =  0.12\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5162817257\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5225490380\n",
      "learning_rate =  0.7\n",
      "alpha =  0.13\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5237487674\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5297001211\n",
      "learning_rate =  0.7\n",
      "alpha =  0.14\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5310775749\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5367162549\n",
      "learning_rate =  0.7\n",
      "alpha =  0.15\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5383016384\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5436307084\n",
      "learning_rate =  0.7\n",
      "alpha =  0.16\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5454480333\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5504703780\n",
      "learning_rate =  0.7\n",
      "alpha =  0.17\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5525391513\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5572574936\n",
      "learning_rate =  0.7\n",
      "alpha =  0.18\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5595939025\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5640108097\n",
      "learning_rate =  0.7\n",
      "alpha =  0.19\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5666285692\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5707464536\n",
      "learning_rate =  0.7\n",
      "alpha =  0.2\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5736574246\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.5774785419\n",
      "learning_rate =  0.7\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5806931884\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5842196365\n",
      "learning_rate =  0.7\n",
      "alpha =  0.22\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.6021368925\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.6043975355\n",
      "learning_rate =  0.7\n",
      "alpha =  0.23\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.6218361011\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.6230334143\n",
      "learning_rate =  0.7\n",
      "alpha =  0.24\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.6375924043\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6380263424\n",
      "learning_rate =  0.7\n",
      "alpha =  0.25\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6519534977\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.6517404422\n",
      "learning_rate =  0.7\n",
      "alpha =  0.26\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6655923342\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6647994762\n",
      "learning_rate =  0.7\n",
      "alpha =  0.27\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6788097052\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6774821710\n",
      "learning_rate =  0.7\n",
      "alpha =  0.28\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6777336986\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6766143073\n",
      "learning_rate =  0.7\n",
      "alpha =  0.29\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6877370652\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6862612268\n",
      "learning_rate =  0.7\n",
      "alpha =  0.3\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6777221704\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6768946694\n",
      "learning_rate =  0.7\n",
      "alpha =  0.31\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.7373220583\n",
      "final testing loss =  0.7340000000\n",
      "final testing accuracy =  0.7337987968\n",
      "learning_rate =  0.7\n",
      "alpha =  0.32\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6497573887\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6507481186\n",
      "learning_rate =  0.7\n",
      "alpha =  0.33\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5383891841\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5490614084\n",
      "learning_rate =  0.7\n",
      "alpha =  0.34\n",
      "final training loss =  0.7800000000\n",
      "final training accuracy =  0.5618399877\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5704259829\n",
      "learning_rate =  0.7\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5690046557\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5778860140\n",
      "learning_rate =  0.7\n",
      "alpha =  0.36\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5355180131\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5450838713\n",
      "learning_rate =  0.7\n",
      "alpha =  0.37\n",
      "final training loss =  0.7740000000\n",
      "final training accuracy =  0.5422586622\n",
      "final testing loss =  0.7740000000\n",
      "final testing accuracy =  0.5514102021\n",
      "learning_rate =  0.7\n",
      "alpha =  0.38\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.9370979182\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.9271344924\n",
      "learning_rate =  0.7\n",
      "alpha =  0.39\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.8209856700\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.8151654262\n",
      "learning_rate =  0.7\n",
      "alpha =  0.4\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6073773416\n",
      "final testing loss =  0.7180000000\n",
      "final testing accuracy =  0.6133287897\n",
      "learning_rate =  0.7\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5721150984\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5808268566\n",
      "learning_rate =  0.7\n",
      "alpha =  0.42\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =  0.6292945507\n",
      "final testing loss =  0.6700000000\n",
      "final testing accuracy =  0.6339503299\n",
      "learning_rate =  0.7\n",
      "alpha =  0.43\n",
      "final training loss =  0.6860000000\n",
      "final training accuracy =  0.6286202635\n",
      "final testing loss =  0.6760000000\n",
      "final testing accuracy =  0.6330994290\n",
      "learning_rate =  0.7\n",
      "alpha =  0.44\n",
      "final training loss =  0.7320000000\n",
      "final training accuracy =  0.7910495331\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.7869963527\n",
      "learning_rate =  0.7\n",
      "alpha =  0.45\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.8052757121\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.8007969609\n",
      "learning_rate =  0.7\n",
      "alpha =  0.46\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  1.0538298190\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =  1.0416550823\n",
      "learning_rate =  0.7\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5509451380\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5602733509\n",
      "learning_rate =  0.7\n",
      "alpha =  0.48\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.7159676560\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.7154264754\n",
      "learning_rate =  0.7\n",
      "alpha =  0.49\n",
      "final training loss =  0.7320000000\n",
      "final training accuracy =  1.1094963406\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =  1.0963168925\n",
      "learning_rate =  0.7\n",
      "alpha =  0.5\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5947357671\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.6019667698\n",
      "learning_rate =  0.7\n",
      "alpha =  0.51\n",
      "final training loss =  0.5280000000\n",
      "final training accuracy =  0.6802144616\n",
      "final testing loss =  0.5400000000\n",
      "final testing accuracy =  0.6819863217\n",
      "learning_rate =  0.7\n",
      "alpha =  0.52\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.8235693510\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.8191303357\n",
      "learning_rate =  0.7\n",
      "alpha =  0.53\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.8655480692\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.8597796092\n",
      "learning_rate =  0.7\n",
      "alpha =  0.54\n",
      "final training loss =  0.7300000000\n",
      "final training accuracy =  1.1316817713\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =  1.1186306952\n",
      "learning_rate =  0.7\n",
      "alpha =  0.55\n",
      "final training loss =  0.7300000000\n",
      "final training accuracy =  1.2325681480\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =  1.2173791677\n",
      "learning_rate =  0.7\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5758626188\n",
      "final testing loss =  0.7640000000\n",
      "final testing accuracy =  0.5847969585\n",
      "learning_rate =  0.7\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6623359231\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6647701320\n",
      "learning_rate =  0.7\n",
      "alpha =  0.58\n",
      "final training loss =  0.5800000000\n",
      "final training accuracy =  0.6777029558\n",
      "final testing loss =  0.5740000000\n",
      "final testing accuracy =  0.6805682502\n",
      "learning_rate =  0.7\n",
      "alpha =  0.59\n",
      "final training loss =  0.7180000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.6\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5564710765\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5644547453\n",
      "learning_rate =  0.7\n",
      "alpha =  0.61\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.7942127729\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.7915617881\n",
      "learning_rate =  0.7\n",
      "alpha =  0.62\n",
      "final training loss =  0.7280000000\n",
      "final training accuracy =  1.2069254468\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =  1.1931357018\n",
      "learning_rate =  0.7\n",
      "alpha =  0.63\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.5756744543\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5850632253\n",
      "learning_rate =  0.7\n",
      "alpha =  0.64\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.6319405437\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6381460405\n",
      "learning_rate =  0.7\n",
      "alpha =  0.65\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6138126101\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.6210307021\n",
      "learning_rate =  0.7\n",
      "alpha =  0.66\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6945922649\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.6960669212\n",
      "learning_rate =  0.7\n",
      "alpha =  0.67\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7462654458\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7441640802\n",
      "learning_rate =  0.7\n",
      "alpha =  0.68\n",
      "final training loss =  0.7140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.6424819545\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6484483346\n",
      "learning_rate =  0.7\n",
      "alpha =  0.71\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5637664765\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5714539053\n",
      "learning_rate =  0.7\n",
      "alpha =  0.72\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5739817196\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5811118547\n",
      "learning_rate =  0.7\n",
      "alpha =  0.73\n",
      "final training loss =  0.7000000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.74\n",
      "final training loss =  0.7180000000\n",
      "final training accuracy =  1.5867505735\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.75\n",
      "final training loss =  0.7200000000\n",
      "final training accuracy =  1.5094320777\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =  1.4915561333\n",
      "learning_rate =  0.7\n",
      "alpha =  0.76\n",
      "final training loss =  0.7140000000\n",
      "final training accuracy =  1.8430822639\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.77\n",
      "final training loss =  0.6980000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.78\n",
      "final training loss =  0.7000000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.79\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.6038397019\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.6099033398\n",
      "learning_rate =  0.7\n",
      "alpha =  0.8\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6099783393\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.6177103160\n",
      "learning_rate =  0.7\n",
      "alpha =  0.81\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.7734537120\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.7727053107\n",
      "learning_rate =  0.7\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.6960000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7020000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5931025871\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6022521380\n",
      "learning_rate =  0.7\n",
      "alpha =  0.84\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6282150620\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.6356908141\n",
      "learning_rate =  0.7\n",
      "alpha =  0.85\n",
      "final training loss =  0.6920000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6900000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.86\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.6115914232\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.6169975624\n",
      "learning_rate =  0.7\n",
      "alpha =  0.87\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7319202737\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7308026311\n",
      "learning_rate =  0.7\n",
      "alpha =  0.88\n",
      "final training loss =  0.7320000000\n",
      "final training accuracy =  0.9427210797\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =  0.9373884503\n",
      "learning_rate =  0.7\n",
      "alpha =  0.89\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  1.0086183455\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.9942887815\n",
      "learning_rate =  0.7\n",
      "alpha =  0.9\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.8175506583\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.8118056542\n",
      "learning_rate =  0.7\n",
      "alpha =  0.91\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6150565807\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.6235252580\n",
      "learning_rate =  0.7\n",
      "alpha =  0.92\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.6273294463\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.6310839479\n",
      "learning_rate =  0.7\n",
      "alpha =  0.93\n",
      "final training loss =  0.6480000000\n",
      "final training accuracy =  0.6721601323\n",
      "final testing loss =  0.6480000000\n",
      "final testing accuracy =  0.6763941882\n",
      "learning_rate =  0.7\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.6860000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6820000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7300000000\n",
      "final training accuracy =  0.9461625009\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =  0.9411037055\n",
      "learning_rate =  0.7\n",
      "alpha =  0.96\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6660000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.97\n",
      "final training loss =  0.6860000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.6840000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.7\n",
      "alpha =  0.98\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.6250687039\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.6327347431\n",
      "learning_rate =  0.7\n",
      "alpha =  0.99\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6009334045\n",
      "final testing loss =  0.7340000000\n",
      "final testing accuracy =  0.6073642765\n",
      "learning_rate =  0.7\n",
      "alpha =  1.0\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5990884339\n",
      "final testing loss =  0.7640000000\n",
      "final testing accuracy =  0.6046198127\n",
      "learning_rate =  0.6\n",
      "alpha =  0.0\n",
      "final training loss =  0.9140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.9040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.6\n",
      "alpha =  0.01\n",
      "final training loss =  0.8600000000\n",
      "final training accuracy =  0.3647149436\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3799221558\n",
      "learning_rate =  0.6\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952629400\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086862372\n",
      "learning_rate =  0.6\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207204\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265627758\n",
      "learning_rate =  0.6\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583377\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396402912\n",
      "learning_rate =  0.6\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499703137\n",
      "learning_rate =  0.6\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099437\n",
      "learning_rate =  0.6\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869621\n",
      "learning_rate =  0.6\n",
      "alpha =  0.08\n",
      "final training loss =  0.7900000000\n",
      "final training accuracy =  0.4615253656\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4723182501\n",
      "learning_rate =  0.6\n",
      "alpha =  0.09\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4683487422\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4787066341\n",
      "learning_rate =  0.6\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4747447545\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4847379420\n",
      "learning_rate =  0.6\n",
      "alpha =  0.11\n",
      "final training loss =  0.7820000000\n",
      "final training accuracy =  0.4808116336\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4904748418\n",
      "learning_rate =  0.6\n",
      "alpha =  0.12\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.4866219389\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.4959771687\n",
      "learning_rate =  0.6\n",
      "alpha =  0.13\n",
      "final training loss =  0.7740000000\n",
      "final training accuracy =  0.4922299926\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5012928406\n",
      "learning_rate =  0.6\n",
      "alpha =  0.14\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.4976773057\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5064595906\n",
      "learning_rate =  0.6\n",
      "alpha =  0.15\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5029962559\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5115073445\n",
      "learning_rate =  0.6\n",
      "alpha =  0.16\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5082125942\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5164601517\n",
      "learning_rate =  0.6\n",
      "alpha =  0.17\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5133471805\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5213376309\n",
      "learning_rate =  0.6\n",
      "alpha =  0.18\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5184172072\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5261560322\n",
      "learning_rate =  0.6\n",
      "alpha =  0.19\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5234370780\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5309290197\n",
      "learning_rate =  0.6\n",
      "alpha =  0.2\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5284190483\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5356682519\n",
      "learning_rate =  0.6\n",
      "alpha =  0.21\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5333736996\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5403838173\n",
      "learning_rate =  0.6\n",
      "alpha =  0.22\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5383102958\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5450845637\n",
      "learning_rate =  0.6\n",
      "alpha =  0.23\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5432370539\n",
      "final testing loss =  0.7640000000\n",
      "final testing accuracy =  0.5497783510\n",
      "learning_rate =  0.6\n",
      "alpha =  0.24\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5481613529\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5544722467\n",
      "learning_rate =  0.6\n",
      "alpha =  0.25\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5530898959\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5591726789\n",
      "learning_rate =  0.6\n",
      "alpha =  0.26\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5580288381\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5638855571\n",
      "learning_rate =  0.6\n",
      "alpha =  0.27\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5629838890\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5686163686\n",
      "learning_rate =  0.6\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5679603935\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.5733702564\n",
      "learning_rate =  0.6\n",
      "alpha =  0.29\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5729633992\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5781520826\n",
      "learning_rate =  0.6\n",
      "alpha =  0.3\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5779977102\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5829664803\n",
      "learning_rate =  0.6\n",
      "alpha =  0.31\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5830679329\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.5878178968\n",
      "learning_rate =  0.6\n",
      "alpha =  0.32\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5881785137\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5927106297\n",
      "learning_rate =  0.6\n",
      "alpha =  0.33\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5933337704\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5976488579\n",
      "learning_rate =  0.6\n",
      "alpha =  0.34\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5985379204\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.6026366675\n",
      "learning_rate =  0.6\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.6037951038\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.6076780746\n",
      "learning_rate =  0.6\n",
      "alpha =  0.36\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.6091094039\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6127770449\n",
      "learning_rate =  0.6\n",
      "alpha =  0.37\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.6144848654\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6179375114\n",
      "learning_rate =  0.6\n",
      "alpha =  0.38\n",
      "final training loss =  0.7480000000\n",
      "final training accuracy =  0.6199255108\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6231633898\n",
      "learning_rate =  0.6\n",
      "alpha =  0.39\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.6254353547\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6284585927\n",
      "learning_rate =  0.6\n",
      "alpha =  0.4\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6507269897\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6524710379\n",
      "learning_rate =  0.6\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6673368203\n",
      "final testing loss =  0.7340000000\n",
      "final testing accuracy =  0.6683262547\n",
      "learning_rate =  0.6\n",
      "alpha =  0.42\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6813254556\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6817238926\n",
      "learning_rate =  0.6\n",
      "alpha =  0.43\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5977593134\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.6027377364\n",
      "learning_rate =  0.6\n",
      "alpha =  0.44\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5959279971\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.6010902126\n",
      "learning_rate =  0.6\n",
      "alpha =  0.45\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5945623274\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5998769690\n",
      "learning_rate =  0.6\n",
      "alpha =  0.46\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5935282683\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5989721967\n",
      "learning_rate =  0.6\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5927421652\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5982974757\n",
      "learning_rate =  0.6\n",
      "alpha =  0.48\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5977126176\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.6030295499\n",
      "learning_rate =  0.6\n",
      "alpha =  0.49\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6097733586\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6144534325\n",
      "learning_rate =  0.6\n",
      "alpha =  0.5\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5804329346\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.5869362269\n",
      "learning_rate =  0.6\n",
      "alpha =  0.51\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5887564480\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5948008328\n",
      "learning_rate =  0.6\n",
      "alpha =  0.52\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6666306110\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6685724757\n",
      "learning_rate =  0.6\n",
      "alpha =  0.53\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.8329581914\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.8282471820\n",
      "learning_rate =  0.6\n",
      "alpha =  0.54\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.8582749730\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.8528108503\n",
      "learning_rate =  0.6\n",
      "alpha =  0.55\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.6232285451\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6275460051\n",
      "learning_rate =  0.6\n",
      "alpha =  0.56\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5859529464\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5923883892\n",
      "learning_rate =  0.6\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5843776749\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5909466375\n",
      "learning_rate =  0.6\n",
      "alpha =  0.58\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6378679430\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6414037279\n",
      "learning_rate =  0.6\n",
      "alpha =  0.59\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5566936030\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5653363494\n",
      "learning_rate =  0.6\n",
      "alpha =  0.6\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.6044773172\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6109129946\n",
      "learning_rate =  0.6\n",
      "alpha =  0.61\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6191423189\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =  0.6245044113\n",
      "learning_rate =  0.6\n",
      "alpha =  0.62\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5821237053\n",
      "final testing loss =  0.7640000000\n",
      "final testing accuracy =  0.5901707805\n",
      "learning_rate =  0.6\n",
      "alpha =  0.63\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5615077530\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5696188972\n",
      "learning_rate =  0.6\n",
      "alpha =  0.64\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.9801159033\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =  0.9719544218\n",
      "learning_rate =  0.6\n",
      "alpha =  0.65\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.5976760820\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6034326488\n",
      "learning_rate =  0.6\n",
      "alpha =  0.66\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.5632997833\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5717697772\n",
      "learning_rate =  0.6\n",
      "alpha =  0.67\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5866677716\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5945350669\n",
      "learning_rate =  0.6\n",
      "alpha =  0.68\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =  0.6445332733\n",
      "final testing loss =  0.6660000000\n",
      "final testing accuracy =  0.6483816199\n",
      "learning_rate =  0.6\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.6240000000\n",
      "final training accuracy =  0.6576195099\n",
      "final testing loss =  0.6320000000\n",
      "final testing accuracy =  0.6600100917\n",
      "learning_rate =  0.6\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.5639798992\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5722490008\n",
      "learning_rate =  0.6\n",
      "alpha =  0.71\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.5693266309\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5784658630\n",
      "learning_rate =  0.6\n",
      "alpha =  0.72\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5826330086\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5900910312\n",
      "learning_rate =  0.6\n",
      "alpha =  0.73\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.6985540866\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.6989569216\n",
      "learning_rate =  0.6\n",
      "alpha =  0.74\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.5809584931\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5896329022\n",
      "learning_rate =  0.6\n",
      "alpha =  0.75\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.5821935600\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5908258062\n",
      "learning_rate =  0.6\n",
      "alpha =  0.76\n",
      "final training loss =  0.7200000000\n",
      "final training accuracy =  1.4360187746\n",
      "final testing loss =  0.7120000000\n",
      "final testing accuracy =  1.4194623720\n",
      "learning_rate =  0.6\n",
      "alpha =  0.77\n",
      "final training loss =  0.5000000000\n",
      "final training accuracy =  0.7093290780\n",
      "final testing loss =  0.5000000000\n",
      "final testing accuracy =  0.7088511810\n",
      "learning_rate =  0.6\n",
      "alpha =  0.78\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.6184470164\n",
      "final testing loss =  0.7340000000\n",
      "final testing accuracy =  0.6238804061\n",
      "learning_rate =  0.6\n",
      "alpha =  0.79\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.5817045140\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5904672736\n",
      "learning_rate =  0.6\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5822386475\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5892494590\n",
      "learning_rate =  0.6\n",
      "alpha =  0.81\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.5768659727\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5859672249\n",
      "learning_rate =  0.6\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.6112231980\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.6169043541\n",
      "learning_rate =  0.6\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7200000000\n",
      "final training accuracy =  1.4572605307\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =  1.4410691037\n",
      "learning_rate =  0.6\n",
      "alpha =  0.84\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.5781576444\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5871333088\n",
      "learning_rate =  0.6\n",
      "alpha =  0.85\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.6157526423\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6211130931\n",
      "learning_rate =  0.6\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.6246138384\n",
      "final testing loss =  0.7340000000\n",
      "final testing accuracy =  0.6294395730\n",
      "learning_rate =  0.6\n",
      "alpha =  0.87\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.6009935391\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.6071913146\n",
      "learning_rate =  0.6\n",
      "alpha =  0.88\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.5816843649\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5906639871\n",
      "learning_rate =  0.6\n",
      "alpha =  0.89\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.7495606379\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.7499756541\n",
      "learning_rate =  0.6\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5724200232\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5797594886\n",
      "learning_rate =  0.6\n",
      "alpha =  0.91\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5834295574\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5924311339\n",
      "learning_rate =  0.6\n",
      "alpha =  0.92\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.6346710111\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.6397209116\n",
      "learning_rate =  0.6\n",
      "alpha =  0.93\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.7618112678\n",
      "final testing loss =  0.7220000000\n",
      "final testing accuracy =  0.7621384355\n",
      "learning_rate =  0.6\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5853982819\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.5923592407\n",
      "learning_rate =  0.6\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.5961065460\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.6046858577\n",
      "learning_rate =  0.6\n",
      "alpha =  0.96\n",
      "final training loss =  0.7280000000\n",
      "final training accuracy =  0.9266152837\n",
      "final testing loss =  0.7140000000\n",
      "final testing accuracy =  0.9221606486\n",
      "learning_rate =  0.6\n",
      "alpha =  0.97\n",
      "final training loss =  0.7000000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.7040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.6\n",
      "alpha =  0.98\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5955121560\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.6018786304\n",
      "learning_rate =  0.6\n",
      "alpha =  0.99\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.5952490983\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.6040099773\n",
      "learning_rate =  0.6\n",
      "alpha =  1.0\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.6084966334\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.6140284998\n",
      "learning_rate =  0.5\n",
      "alpha =  0.0\n",
      "final training loss =  0.9140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.9040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.5\n",
      "alpha =  0.01\n",
      "final training loss =  0.8600000000\n",
      "final training accuracy =  0.3647149436\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3799221558\n",
      "learning_rate =  0.5\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952629400\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086862372\n",
      "learning_rate =  0.5\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207204\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265627758\n",
      "learning_rate =  0.5\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583377\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396402912\n",
      "learning_rate =  0.5\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499703137\n",
      "learning_rate =  0.5\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099437\n",
      "learning_rate =  0.5\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869621\n",
      "learning_rate =  0.5\n",
      "alpha =  0.08\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4608863420\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721243211\n",
      "learning_rate =  0.5\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666526016\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777343361\n",
      "learning_rate =  0.5\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4718246164\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827641655\n",
      "learning_rate =  0.5\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765118284\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873200553\n",
      "learning_rate =  0.5\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807956019\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914812545\n",
      "learning_rate =  0.5\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381117\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953084749\n",
      "learning_rate =  0.5\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883879714\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988492767\n",
      "learning_rate =  0.5\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839373\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021416297\n",
      "learning_rate =  0.5\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574274\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052163458\n",
      "learning_rate =  0.5\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342819\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080987837\n",
      "learning_rate =  0.5\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360246\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108100755\n",
      "learning_rate =  0.5\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807851\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133680246\n",
      "learning_rate =  0.5\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839865\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157877770\n",
      "learning_rate =  0.5\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588656\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180823307\n",
      "learning_rate =  0.5\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202629279\n",
      "learning_rate =  0.5\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393614\n",
      "learning_rate =  0.5\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202160\n",
      "learning_rate =  0.5\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130617\n",
      "learning_rate =  0.5\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246096\n",
      "learning_rate =  0.5\n",
      "alpha =  0.27\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5213658774\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5304743004\n",
      "learning_rate =  0.5\n",
      "alpha =  0.28\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5243365171\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5332141553\n",
      "learning_rate =  0.5\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5273345369\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5360087231\n",
      "learning_rate =  0.5\n",
      "alpha =  0.3\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5303529525\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5388376434\n",
      "learning_rate =  0.5\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5333894643\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5416935970\n",
      "learning_rate =  0.5\n",
      "alpha =  0.32\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5364436184\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5445735615\n",
      "learning_rate =  0.5\n",
      "alpha =  0.33\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5395158608\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5474764421\n",
      "learning_rate =  0.5\n",
      "alpha =  0.34\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5426071271\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5504021533\n",
      "learning_rate =  0.5\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5457186367\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5533511899\n",
      "learning_rate =  0.5\n",
      "alpha =  0.36\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5488517815\n",
      "final testing loss =  0.7640000000\n",
      "final testing accuracy =  0.5563244027\n",
      "learning_rate =  0.5\n",
      "alpha =  0.37\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5520080612\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5593228707\n",
      "learning_rate =  0.5\n",
      "alpha =  0.38\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5551890442\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5623478257\n",
      "learning_rate =  0.5\n",
      "alpha =  0.39\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5583963443\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5654006043\n",
      "learning_rate =  0.5\n",
      "alpha =  0.4\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5616316057\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5684826186\n",
      "learning_rate =  0.5\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5648964944\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5715953359\n",
      "learning_rate =  0.5\n",
      "alpha =  0.42\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5681926926\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5747402661\n",
      "learning_rate =  0.5\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5715218962\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5779189536\n",
      "learning_rate =  0.5\n",
      "alpha =  0.44\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5748858132\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5811329717\n",
      "learning_rate =  0.5\n",
      "alpha =  0.45\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5782861633\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.5843839194\n",
      "learning_rate =  0.5\n",
      "alpha =  0.46\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5817246792\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5876734195\n",
      "learning_rate =  0.5\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5852031068\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5910031180\n",
      "learning_rate =  0.5\n",
      "alpha =  0.48\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5887232071\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5943746839\n",
      "learning_rate =  0.5\n",
      "alpha =  0.49\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5922867577\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5977898097\n",
      "learning_rate =  0.5\n",
      "alpha =  0.5\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5958955547\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6012502121\n",
      "learning_rate =  0.5\n",
      "alpha =  0.51\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.5995514146\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6047576335\n",
      "learning_rate =  0.5\n",
      "alpha =  0.52\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6032561763\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6083138432\n",
      "learning_rate =  0.5\n",
      "alpha =  0.53\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6070117034\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6119206388\n",
      "learning_rate =  0.5\n",
      "alpha =  0.54\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6108198863\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6155798485\n",
      "learning_rate =  0.5\n",
      "alpha =  0.55\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6146826444\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6192933321\n",
      "learning_rate =  0.5\n",
      "alpha =  0.56\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6186019283\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.6230629839\n",
      "learning_rate =  0.5\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6225797225\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.6268907340\n",
      "learning_rate =  0.5\n",
      "alpha =  0.58\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6266180471\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.6307785505\n",
      "learning_rate =  0.5\n",
      "alpha =  0.59\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6307189611\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6347284421\n",
      "learning_rate =  0.5\n",
      "alpha =  0.6\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6348845639\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.6387424601\n",
      "learning_rate =  0.5\n",
      "alpha =  0.61\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6391169987\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6428227006\n",
      "learning_rate =  0.5\n",
      "alpha =  0.62\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6434184542\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6469713072\n",
      "learning_rate =  0.5\n",
      "alpha =  0.63\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6477911681\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6511904732\n",
      "learning_rate =  0.5\n",
      "alpha =  0.64\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6522374292\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6554824444\n",
      "learning_rate =  0.5\n",
      "alpha =  0.65\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6567595801\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6598495217\n",
      "learning_rate =  0.5\n",
      "alpha =  0.66\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.6613600205\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6642940638\n",
      "learning_rate =  0.5\n",
      "alpha =  0.67\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.6660412098\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6688184899\n",
      "learning_rate =  0.5\n",
      "alpha =  0.68\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6708056702\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6734252829\n",
      "learning_rate =  0.5\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6756559898\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.6781169922\n",
      "learning_rate =  0.5\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6805948255\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.6828962369\n",
      "learning_rate =  0.5\n",
      "alpha =  0.71\n",
      "final training loss =  0.7320000000\n",
      "final training accuracy =  0.7057864233\n",
      "final testing loss =  0.7200000000\n",
      "final testing accuracy =  0.7071000021\n",
      "learning_rate =  0.5\n",
      "alpha =  0.72\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.7237679106\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.7244257324\n",
      "learning_rate =  0.5\n",
      "alpha =  0.73\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6513604340\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6550648671\n",
      "learning_rate =  0.5\n",
      "alpha =  0.74\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6477388749\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6516439216\n",
      "learning_rate =  0.5\n",
      "alpha =  0.75\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6449727022\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6490390144\n",
      "learning_rate =  0.5\n",
      "alpha =  0.76\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.7769791090\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.7759103271\n",
      "learning_rate =  0.5\n",
      "alpha =  0.77\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.6176060830\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.6225068126\n",
      "learning_rate =  0.5\n",
      "alpha =  0.78\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5876226131\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5948340698\n",
      "learning_rate =  0.5\n",
      "alpha =  0.79\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.6222898888\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6268788323\n",
      "learning_rate =  0.5\n",
      "alpha =  0.8\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.6245759239\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.6290142788\n",
      "learning_rate =  0.5\n",
      "alpha =  0.81\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.6268383489\n",
      "final testing loss =  0.7400000000\n",
      "final testing accuracy =  0.6311286776\n",
      "learning_rate =  0.5\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5879598590\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5952329232\n",
      "learning_rate =  0.5\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5881282896\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5954107326\n",
      "learning_rate =  0.5\n",
      "alpha =  0.84\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5883212519\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5956113566\n",
      "learning_rate =  0.5\n",
      "alpha =  0.85\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5885357688\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5958320441\n",
      "learning_rate =  0.5\n",
      "alpha =  0.86\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5887694186\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5960705579\n",
      "learning_rate =  0.5\n",
      "alpha =  0.87\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5912829421\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5984087986\n",
      "learning_rate =  0.5\n",
      "alpha =  0.88\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5935752950\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.6005465421\n",
      "learning_rate =  0.5\n",
      "alpha =  0.89\n",
      "final training loss =  0.7500000000\n",
      "final training accuracy =  0.5865957463\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5941556918\n",
      "learning_rate =  0.5\n",
      "alpha =  0.9\n",
      "final training loss =  0.6820000000\n",
      "final training accuracy =  0.6521127218\n",
      "final testing loss =  0.6700000000\n",
      "final testing accuracy =  0.6549492217\n",
      "learning_rate =  0.5\n",
      "alpha =  0.91\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6400433021\n",
      "final testing loss =  0.7100000000\n",
      "final testing accuracy =  0.6434000341\n",
      "learning_rate =  0.5\n",
      "alpha =  0.92\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.6393748183\n",
      "final testing loss =  0.7160000000\n",
      "final testing accuracy =  0.6427516135\n",
      "learning_rate =  0.5\n",
      "alpha =  0.93\n",
      "final training loss =  0.6600000000\n",
      "final training accuracy =  0.6590867714\n",
      "final testing loss =  0.6520000000\n",
      "final testing accuracy =  0.6615929741\n",
      "learning_rate =  0.5\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.6600000000\n",
      "final training accuracy =  0.6587223467\n",
      "final testing loss =  0.6520000000\n",
      "final testing accuracy =  0.6610553109\n",
      "learning_rate =  0.5\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.6391300851\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.6424815759\n",
      "learning_rate =  0.5\n",
      "alpha =  0.96\n",
      "final training loss =  0.6500000000\n",
      "final training accuracy =  0.6609898792\n",
      "final testing loss =  0.6520000000\n",
      "final testing accuracy =  0.6631623988\n",
      "learning_rate =  0.5\n",
      "alpha =  0.97\n",
      "final training loss =  0.5840000000\n",
      "final training accuracy =  0.6745182828\n",
      "final testing loss =  0.5860000000\n",
      "final testing accuracy =  0.6760856684\n",
      "learning_rate =  0.5\n",
      "alpha =  0.98\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.6370249414\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.6404636023\n",
      "learning_rate =  0.5\n",
      "alpha =  0.99\n",
      "final training loss =  0.6620000000\n",
      "final training accuracy =  0.6594927892\n",
      "final testing loss =  0.6560000000\n",
      "final testing accuracy =  0.6616968607\n",
      "learning_rate =  0.5\n",
      "alpha =  1.0\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.6180034668\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6234369053\n",
      "learning_rate =  0.4\n",
      "alpha =  0.0\n",
      "final training loss =  0.9140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.9060000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.4\n",
      "alpha =  0.01\n",
      "final training loss =  0.8600000000\n",
      "final training accuracy =  0.3647149436\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3799221558\n",
      "learning_rate =  0.4\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952629400\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086862372\n",
      "learning_rate =  0.4\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207204\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265627758\n",
      "learning_rate =  0.4\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583377\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396402912\n",
      "learning_rate =  0.4\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499703137\n",
      "learning_rate =  0.4\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099437\n",
      "learning_rate =  0.4\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869621\n",
      "learning_rate =  0.4\n",
      "alpha =  0.08\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4608863420\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721243211\n",
      "learning_rate =  0.4\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666526016\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777343361\n",
      "learning_rate =  0.4\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4718246164\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827641655\n",
      "learning_rate =  0.4\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765118284\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873200553\n",
      "learning_rate =  0.4\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807956019\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914812545\n",
      "learning_rate =  0.4\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381117\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953084749\n",
      "learning_rate =  0.4\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883879714\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988492767\n",
      "learning_rate =  0.4\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839373\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021416297\n",
      "learning_rate =  0.4\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574274\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052163458\n",
      "learning_rate =  0.4\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342819\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080987837\n",
      "learning_rate =  0.4\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360246\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108100755\n",
      "learning_rate =  0.4\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807851\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133680246\n",
      "learning_rate =  0.4\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839865\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157877770\n",
      "learning_rate =  0.4\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588656\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180823307\n",
      "learning_rate =  0.4\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202629279\n",
      "learning_rate =  0.4\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393614\n",
      "learning_rate =  0.4\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202160\n",
      "learning_rate =  0.4\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130617\n",
      "learning_rate =  0.4\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246096\n",
      "learning_rate =  0.4\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203639609\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297608379\n",
      "learning_rate =  0.4\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934362\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314270966\n",
      "learning_rate =  0.4\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558085\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330281931\n",
      "learning_rate =  0.4\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555102\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345684635\n",
      "learning_rate =  0.4\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965562\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518334\n",
      "learning_rate =  0.4\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283825946\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818681\n",
      "learning_rate =  0.4\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169510\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618156\n",
      "learning_rate =  0.4\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026654\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946437\n",
      "learning_rate =  0.4\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425241\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830709\n",
      "learning_rate =  0.4\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390877\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427295945\n",
      "learning_rate =  0.4\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947146\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365129\n",
      "learning_rate =  0.4\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115820\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059474\n",
      "learning_rate =  0.4\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398589\n",
      "learning_rate =  0.4\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369468\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400643\n",
      "learning_rate =  0.4\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490443\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082499\n",
      "learning_rate =  0.4\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296091\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459836\n",
      "learning_rate =  0.4\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547260\n",
      "learning_rate =  0.4\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358394\n",
      "learning_rate =  0.4\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905968\n",
      "learning_rate =  0.4\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201892\n",
      "learning_rate =  0.4\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257325\n",
      "learning_rate =  0.4\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082737\n",
      "learning_rate =  0.4\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.4\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082252\n",
      "learning_rate =  0.4\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.4\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.4\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.4\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716968\n",
      "learning_rate =  0.4\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.4\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.4\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.4\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.4\n",
      "alpha =  0.59\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5558059859\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5637867230\n",
      "learning_rate =  0.4\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5574745703\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5653545758\n",
      "learning_rate =  0.4\n",
      "alpha =  0.61\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5592003226\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5669884894\n",
      "learning_rate =  0.4\n",
      "alpha =  0.62\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5609704259\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5686697212\n",
      "learning_rate =  0.4\n",
      "alpha =  0.63\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5627795850\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5703915424\n",
      "learning_rate =  0.4\n",
      "alpha =  0.64\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5646249454\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5721504729\n",
      "learning_rate =  0.4\n",
      "alpha =  0.65\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5665048236\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.5739444845\n",
      "learning_rate =  0.4\n",
      "alpha =  0.66\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5684182052\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5757723452\n",
      "learning_rate =  0.4\n",
      "alpha =  0.67\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5703645010\n",
      "final testing loss =  0.7460000000\n",
      "final testing accuracy =  0.5776333153\n",
      "learning_rate =  0.4\n",
      "alpha =  0.68\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5723434116\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5795269842\n",
      "learning_rate =  0.4\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5743548465\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5814531747\n",
      "learning_rate =  0.4\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5763988718\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5834118830\n",
      "learning_rate =  0.4\n",
      "alpha =  0.71\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5784756752\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5854032382\n",
      "learning_rate =  0.4\n",
      "alpha =  0.72\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5805855420\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5874274752\n",
      "learning_rate =  0.4\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5827288372\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5894849151\n",
      "learning_rate =  0.4\n",
      "alpha =  0.74\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5849059931\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.5915759504\n",
      "learning_rate =  0.4\n",
      "alpha =  0.75\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5871174992\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.5937010352\n",
      "learning_rate =  0.4\n",
      "alpha =  0.76\n",
      "final training loss =  0.7460000000\n",
      "final training accuracy =  0.5893638956\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.5958606765\n",
      "learning_rate =  0.4\n",
      "alpha =  0.77\n",
      "final training loss =  0.7440000000\n",
      "final training accuracy =  0.5916457667\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.5980554281\n",
      "learning_rate =  0.4\n",
      "alpha =  0.78\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.5939637374\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6002858865\n",
      "learning_rate =  0.4\n",
      "alpha =  0.79\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.5963184694\n",
      "final testing loss =  0.7420000000\n",
      "final testing accuracy =  0.6025526865\n",
      "learning_rate =  0.4\n",
      "alpha =  0.8\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5987106590\n",
      "final testing loss =  0.7440000000\n",
      "final testing accuracy =  0.6048564984\n",
      "learning_rate =  0.4\n",
      "alpha =  0.81\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6011410345\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.6071980262\n",
      "learning_rate =  0.4\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6036103552\n",
      "final testing loss =  0.7340000000\n",
      "final testing accuracy =  0.6095780055\n",
      "learning_rate =  0.4\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6061194098\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.6119972024\n",
      "learning_rate =  0.4\n",
      "alpha =  0.84\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6086690157\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.6144564120\n",
      "learning_rate =  0.4\n",
      "alpha =  0.85\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6112600181\n",
      "final testing loss =  0.7380000000\n",
      "final testing accuracy =  0.6169564583\n",
      "learning_rate =  0.4\n",
      "alpha =  0.86\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6138932900\n",
      "final testing loss =  0.7360000000\n",
      "final testing accuracy =  0.6194981932\n",
      "learning_rate =  0.4\n",
      "alpha =  0.87\n",
      "final training loss =  0.7420000000\n",
      "final training accuracy =  0.6165697316\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6220824967\n",
      "learning_rate =  0.4\n",
      "alpha =  0.88\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6192902702\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6247102759\n",
      "learning_rate =  0.4\n",
      "alpha =  0.89\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6220558604\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6273824658\n",
      "learning_rate =  0.4\n",
      "alpha =  0.9\n",
      "final training loss =  0.7400000000\n",
      "final training accuracy =  0.6248674841\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6301000289\n",
      "learning_rate =  0.4\n",
      "alpha =  0.91\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.6277261506\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.6328639557\n",
      "learning_rate =  0.4\n",
      "alpha =  0.92\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.6306328973\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6356752645\n",
      "learning_rate =  0.4\n",
      "alpha =  0.93\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.6335887897\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.6385350024\n",
      "learning_rate =  0.4\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5965385234\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.6025303444\n",
      "learning_rate =  0.4\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5975887348\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.6035220466\n",
      "learning_rate =  0.4\n",
      "alpha =  0.96\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5986386282\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.6045133542\n",
      "learning_rate =  0.4\n",
      "alpha =  0.97\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5996885606\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.6055046124\n",
      "learning_rate =  0.4\n",
      "alpha =  0.98\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.6007388849\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.6064961626\n",
      "learning_rate =  0.4\n",
      "alpha =  0.99\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.6017899514\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.6074883430\n",
      "learning_rate =  0.4\n",
      "alpha =  1.0\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.6028421076\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.6084814893\n",
      "learning_rate =  0.3\n",
      "alpha =  0.0\n",
      "final training loss =  0.9140000000\n",
      "final training accuracy =           nan\n",
      "final testing loss =  0.9040000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.3\n",
      "alpha =  0.01\n",
      "final training loss =  0.8600000000\n",
      "final training accuracy =  0.3647149436\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3799221558\n",
      "learning_rate =  0.3\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952629400\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086862372\n",
      "learning_rate =  0.3\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207204\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265627758\n",
      "learning_rate =  0.3\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583377\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396402912\n",
      "learning_rate =  0.3\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499703137\n",
      "learning_rate =  0.3\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099437\n",
      "learning_rate =  0.3\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869621\n",
      "learning_rate =  0.3\n",
      "alpha =  0.08\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4608863420\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721243211\n",
      "learning_rate =  0.3\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666526016\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777343361\n",
      "learning_rate =  0.3\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4718246164\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827641655\n",
      "learning_rate =  0.3\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765118284\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873200553\n",
      "learning_rate =  0.3\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807956019\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914812545\n",
      "learning_rate =  0.3\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381117\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953084749\n",
      "learning_rate =  0.3\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883879714\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988492767\n",
      "learning_rate =  0.3\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839373\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021416297\n",
      "learning_rate =  0.3\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574274\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052163458\n",
      "learning_rate =  0.3\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342819\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080987837\n",
      "learning_rate =  0.3\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360246\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108100755\n",
      "learning_rate =  0.3\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807851\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133680246\n",
      "learning_rate =  0.3\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839865\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157877770\n",
      "learning_rate =  0.3\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588656\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180823307\n",
      "learning_rate =  0.3\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202629279\n",
      "learning_rate =  0.3\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393614\n",
      "learning_rate =  0.3\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202160\n",
      "learning_rate =  0.3\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130617\n",
      "learning_rate =  0.3\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246096\n",
      "learning_rate =  0.3\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203639609\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297608379\n",
      "learning_rate =  0.3\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934362\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314270966\n",
      "learning_rate =  0.3\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558085\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330281931\n",
      "learning_rate =  0.3\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555102\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345684635\n",
      "learning_rate =  0.3\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965562\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518334\n",
      "learning_rate =  0.3\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283825946\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818681\n",
      "learning_rate =  0.3\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169510\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618156\n",
      "learning_rate =  0.3\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026654\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946437\n",
      "learning_rate =  0.3\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425241\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830709\n",
      "learning_rate =  0.3\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390877\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427295945\n",
      "learning_rate =  0.3\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947146\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365129\n",
      "learning_rate =  0.3\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115820\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059474\n",
      "learning_rate =  0.3\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398589\n",
      "learning_rate =  0.3\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369468\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400643\n",
      "learning_rate =  0.3\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490443\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082499\n",
      "learning_rate =  0.3\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296091\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459836\n",
      "learning_rate =  0.3\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547260\n",
      "learning_rate =  0.3\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358394\n",
      "learning_rate =  0.3\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905968\n",
      "learning_rate =  0.3\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201892\n",
      "learning_rate =  0.3\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257325\n",
      "learning_rate =  0.3\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082737\n",
      "learning_rate =  0.3\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.3\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082252\n",
      "learning_rate =  0.3\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.3\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.3\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.3\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716968\n",
      "learning_rate =  0.3\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.3\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.3\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.3\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.3\n",
      "alpha =  0.59\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5555227329\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5635426576\n",
      "learning_rate =  0.3\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5562208607\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642118885\n",
      "learning_rate =  0.3\n",
      "alpha =  0.61\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5569046896\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648673718\n",
      "learning_rate =  0.3\n",
      "alpha =  0.62\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5575747159\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5655095865\n",
      "learning_rate =  0.3\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582314121\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5661389889\n",
      "learning_rate =  0.3\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5588752284\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5667560134\n",
      "learning_rate =  0.3\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595065940\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5673610742\n",
      "learning_rate =  0.3\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601259183\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5679545662\n",
      "learning_rate =  0.3\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607335923\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5685368665\n",
      "learning_rate =  0.3\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613299894\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5691083352\n",
      "learning_rate =  0.3\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619154665\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5696693164\n",
      "learning_rate =  0.3\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5624903651\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702201391\n",
      "learning_rate =  0.3\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630550119\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707611182\n",
      "learning_rate =  0.3\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636097196\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712925550\n",
      "learning_rate =  0.3\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5641547880\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5718147382\n",
      "learning_rate =  0.3\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5646905041\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5723279444\n",
      "learning_rate =  0.3\n",
      "alpha =  0.75\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5652171436\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728324389\n",
      "learning_rate =  0.3\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657349707\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733284760\n",
      "learning_rate =  0.3\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662442390\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738163000\n",
      "learning_rate =  0.3\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667451923\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742961454\n",
      "learning_rate =  0.3\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672380647\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747682374\n",
      "learning_rate =  0.3\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677230811\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752327926\n",
      "learning_rate =  0.3\n",
      "alpha =  0.81\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5682004580\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5756900191\n",
      "learning_rate =  0.3\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686704035\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5761401171\n",
      "learning_rate =  0.3\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691331182\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765832795\n",
      "learning_rate =  0.3\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695887950\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5770196917\n",
      "learning_rate =  0.3\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700376197\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774495325\n",
      "learning_rate =  0.3\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704797715\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778729740\n",
      "learning_rate =  0.3\n",
      "alpha =  0.87\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5709154230\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782901822\n",
      "learning_rate =  0.3\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713447409\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5787013171\n",
      "learning_rate =  0.3\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717678856\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5791065331\n",
      "learning_rate =  0.3\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721850124\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5795059792\n",
      "learning_rate =  0.3\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5725962708\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5798997992\n",
      "learning_rate =  0.3\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730018054\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5802881319\n",
      "learning_rate =  0.3\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734017559\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5806711114\n",
      "learning_rate =  0.3\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737962572\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810488673\n",
      "learning_rate =  0.3\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741854397\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814215250\n",
      "learning_rate =  0.3\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745694298\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5817892055\n",
      "learning_rate =  0.3\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749483493\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821520260\n",
      "learning_rate =  0.3\n",
      "alpha =  0.98\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5753223163\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825100999\n",
      "learning_rate =  0.3\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5756914452\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5828635368\n",
      "learning_rate =  0.3\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760558466\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832124430\n",
      "learning_rate =  0.2\n",
      "alpha =  0.0\n",
      "final training loss =  0.9180000000\n",
      "final training accuracy =  0.2055318316\n",
      "final testing loss =  0.9060000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.2\n",
      "alpha =  0.01\n",
      "final training loss =  0.8600000000\n",
      "final training accuracy =  0.3647149436\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3799221558\n",
      "learning_rate =  0.2\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952629400\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086862372\n",
      "learning_rate =  0.2\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207204\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265627758\n",
      "learning_rate =  0.2\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583377\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396402912\n",
      "learning_rate =  0.2\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499703137\n",
      "learning_rate =  0.2\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099437\n",
      "learning_rate =  0.2\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869621\n",
      "learning_rate =  0.2\n",
      "alpha =  0.08\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4608863420\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721243211\n",
      "learning_rate =  0.2\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666526016\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777343361\n",
      "learning_rate =  0.2\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4718246164\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827641655\n",
      "learning_rate =  0.2\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765118284\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873200553\n",
      "learning_rate =  0.2\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807956019\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914812545\n",
      "learning_rate =  0.2\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381117\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953084749\n",
      "learning_rate =  0.2\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883879714\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988492767\n",
      "learning_rate =  0.2\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839373\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021416297\n",
      "learning_rate =  0.2\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574274\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052163458\n",
      "learning_rate =  0.2\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342819\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080987837\n",
      "learning_rate =  0.2\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360246\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108100755\n",
      "learning_rate =  0.2\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807851\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133680246\n",
      "learning_rate =  0.2\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839865\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157877770\n",
      "learning_rate =  0.2\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588656\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180823307\n",
      "learning_rate =  0.2\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202629279\n",
      "learning_rate =  0.2\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393614\n",
      "learning_rate =  0.2\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202160\n",
      "learning_rate =  0.2\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130617\n",
      "learning_rate =  0.2\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246096\n",
      "learning_rate =  0.2\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203639609\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297608379\n",
      "learning_rate =  0.2\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934362\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314270966\n",
      "learning_rate =  0.2\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558085\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330281931\n",
      "learning_rate =  0.2\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555102\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345684635\n",
      "learning_rate =  0.2\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965562\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518334\n",
      "learning_rate =  0.2\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283825946\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818681\n",
      "learning_rate =  0.2\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169510\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618156\n",
      "learning_rate =  0.2\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026654\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946437\n",
      "learning_rate =  0.2\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425241\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830709\n",
      "learning_rate =  0.2\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390877\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427295945\n",
      "learning_rate =  0.2\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947146\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365129\n",
      "learning_rate =  0.2\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115820\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059474\n",
      "learning_rate =  0.2\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398589\n",
      "learning_rate =  0.2\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369468\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400643\n",
      "learning_rate =  0.2\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490443\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082499\n",
      "learning_rate =  0.2\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296091\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459836\n",
      "learning_rate =  0.2\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547260\n",
      "learning_rate =  0.2\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358394\n",
      "learning_rate =  0.2\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905968\n",
      "learning_rate =  0.2\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201892\n",
      "learning_rate =  0.2\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257325\n",
      "learning_rate =  0.2\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082737\n",
      "learning_rate =  0.2\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.2\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082252\n",
      "learning_rate =  0.2\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.2\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.2\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.2\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716968\n",
      "learning_rate =  0.2\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.2\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.2\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.2\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.2\n",
      "alpha =  0.59\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5555227329\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5635426576\n",
      "learning_rate =  0.2\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5562208607\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642118885\n",
      "learning_rate =  0.2\n",
      "alpha =  0.61\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5569046896\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648673718\n",
      "learning_rate =  0.2\n",
      "alpha =  0.62\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5575747159\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5655095865\n",
      "learning_rate =  0.2\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582314121\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5661389889\n",
      "learning_rate =  0.2\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5588752284\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5667560134\n",
      "learning_rate =  0.2\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595065940\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5673610742\n",
      "learning_rate =  0.2\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601259183\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5679545662\n",
      "learning_rate =  0.2\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607335923\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5685368665\n",
      "learning_rate =  0.2\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613299894\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5691083352\n",
      "learning_rate =  0.2\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619154665\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5696693164\n",
      "learning_rate =  0.2\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5624903651\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702201391\n",
      "learning_rate =  0.2\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630550119\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707611182\n",
      "learning_rate =  0.2\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636097196\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712925550\n",
      "learning_rate =  0.2\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5641547880\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5718147382\n",
      "learning_rate =  0.2\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5646905041\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5723279444\n",
      "learning_rate =  0.2\n",
      "alpha =  0.75\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5652171436\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728324389\n",
      "learning_rate =  0.2\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657349707\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733284760\n",
      "learning_rate =  0.2\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662442390\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738163000\n",
      "learning_rate =  0.2\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667451923\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742961454\n",
      "learning_rate =  0.2\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672380647\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747682374\n",
      "learning_rate =  0.2\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677230811\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752327926\n",
      "learning_rate =  0.2\n",
      "alpha =  0.81\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5682004580\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5756900191\n",
      "learning_rate =  0.2\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686704035\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5761401171\n",
      "learning_rate =  0.2\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691331182\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765832795\n",
      "learning_rate =  0.2\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695887950\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5770196917\n",
      "learning_rate =  0.2\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700376197\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774495325\n",
      "learning_rate =  0.2\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704797715\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778729740\n",
      "learning_rate =  0.2\n",
      "alpha =  0.87\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5709154230\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782901822\n",
      "learning_rate =  0.2\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713447409\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5787013171\n",
      "learning_rate =  0.2\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717678856\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5791065331\n",
      "learning_rate =  0.2\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721850124\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5795059792\n",
      "learning_rate =  0.2\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5725962708\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5798997992\n",
      "learning_rate =  0.2\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730018054\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5802881319\n",
      "learning_rate =  0.2\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734017559\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5806711114\n",
      "learning_rate =  0.2\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737962572\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810488673\n",
      "learning_rate =  0.2\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741854397\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814215250\n",
      "learning_rate =  0.2\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745694298\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5817892055\n",
      "learning_rate =  0.2\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749483493\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821520260\n",
      "learning_rate =  0.2\n",
      "alpha =  0.98\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5753223163\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825100999\n",
      "learning_rate =  0.2\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5756914452\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5828635368\n",
      "learning_rate =  0.2\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760558466\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832124430\n",
      "learning_rate =  0.1\n",
      "alpha =  0.0\n",
      "final training loss =  0.9180000000\n",
      "final training accuracy =  0.2113019574\n",
      "final testing loss =  0.9020000000\n",
      "final testing accuracy =           nan\n",
      "learning_rate =  0.1\n",
      "alpha =  0.01\n",
      "final training loss =  0.8600000000\n",
      "final training accuracy =  0.3647149436\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3799221558\n",
      "learning_rate =  0.1\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952629400\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086862372\n",
      "learning_rate =  0.1\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207204\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265627758\n",
      "learning_rate =  0.1\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583377\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396402912\n",
      "learning_rate =  0.1\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499703137\n",
      "learning_rate =  0.1\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099437\n",
      "learning_rate =  0.1\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869621\n",
      "learning_rate =  0.1\n",
      "alpha =  0.08\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4608863420\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721243211\n",
      "learning_rate =  0.1\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666526016\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777343361\n",
      "learning_rate =  0.1\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4718246164\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827641655\n",
      "learning_rate =  0.1\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765118284\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873200553\n",
      "learning_rate =  0.1\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807956019\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914812545\n",
      "learning_rate =  0.1\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381117\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953084749\n",
      "learning_rate =  0.1\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883879714\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988492767\n",
      "learning_rate =  0.1\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839373\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021416297\n",
      "learning_rate =  0.1\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574274\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052163458\n",
      "learning_rate =  0.1\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342819\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080987837\n",
      "learning_rate =  0.1\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360246\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108100755\n",
      "learning_rate =  0.1\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807851\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133680246\n",
      "learning_rate =  0.1\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839865\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157877770\n",
      "learning_rate =  0.1\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588656\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180823307\n",
      "learning_rate =  0.1\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202629279\n",
      "learning_rate =  0.1\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393614\n",
      "learning_rate =  0.1\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202160\n",
      "learning_rate =  0.1\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130617\n",
      "learning_rate =  0.1\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246096\n",
      "learning_rate =  0.1\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203639609\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297608379\n",
      "learning_rate =  0.1\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934362\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314270966\n",
      "learning_rate =  0.1\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558085\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330281931\n",
      "learning_rate =  0.1\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555102\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345684635\n",
      "learning_rate =  0.1\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965562\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518334\n",
      "learning_rate =  0.1\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283825946\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818681\n",
      "learning_rate =  0.1\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169510\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618156\n",
      "learning_rate =  0.1\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026654\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946437\n",
      "learning_rate =  0.1\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425241\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830709\n",
      "learning_rate =  0.1\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390877\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427295945\n",
      "learning_rate =  0.1\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947146\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365129\n",
      "learning_rate =  0.1\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115820\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059474\n",
      "learning_rate =  0.1\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398589\n",
      "learning_rate =  0.1\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369468\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400643\n",
      "learning_rate =  0.1\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490443\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082499\n",
      "learning_rate =  0.1\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296091\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459836\n",
      "learning_rate =  0.1\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547260\n",
      "learning_rate =  0.1\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358394\n",
      "learning_rate =  0.1\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905968\n",
      "learning_rate =  0.1\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201892\n",
      "learning_rate =  0.1\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257325\n",
      "learning_rate =  0.1\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082737\n",
      "learning_rate =  0.1\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.1\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082252\n",
      "learning_rate =  0.1\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.1\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.1\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.1\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716968\n",
      "learning_rate =  0.1\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.1\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.1\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.1\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.1\n",
      "alpha =  0.59\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5555227329\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5635426576\n",
      "learning_rate =  0.1\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5562208607\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642118885\n",
      "learning_rate =  0.1\n",
      "alpha =  0.61\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5569046896\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648673718\n",
      "learning_rate =  0.1\n",
      "alpha =  0.62\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5575747159\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5655095865\n",
      "learning_rate =  0.1\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582314121\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5661389889\n",
      "learning_rate =  0.1\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5588752284\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5667560134\n",
      "learning_rate =  0.1\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595065940\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5673610742\n",
      "learning_rate =  0.1\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601259183\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5679545662\n",
      "learning_rate =  0.1\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607335923\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5685368665\n",
      "learning_rate =  0.1\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613299894\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5691083352\n",
      "learning_rate =  0.1\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619154665\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5696693164\n",
      "learning_rate =  0.1\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5624903651\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702201391\n",
      "learning_rate =  0.1\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630550119\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707611182\n",
      "learning_rate =  0.1\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636097196\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712925550\n",
      "learning_rate =  0.1\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5641547880\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5718147382\n",
      "learning_rate =  0.1\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5646905041\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5723279444\n",
      "learning_rate =  0.1\n",
      "alpha =  0.75\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5652171436\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728324389\n",
      "learning_rate =  0.1\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657349707\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733284760\n",
      "learning_rate =  0.1\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662442390\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738163000\n",
      "learning_rate =  0.1\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667451923\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742961454\n",
      "learning_rate =  0.1\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672380647\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747682374\n",
      "learning_rate =  0.1\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677230811\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752327926\n",
      "learning_rate =  0.1\n",
      "alpha =  0.81\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5682004580\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5756900191\n",
      "learning_rate =  0.1\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686704035\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5761401171\n",
      "learning_rate =  0.1\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691331182\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765832795\n",
      "learning_rate =  0.1\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695887950\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5770196917\n",
      "learning_rate =  0.1\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700376197\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774495325\n",
      "learning_rate =  0.1\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704797715\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778729740\n",
      "learning_rate =  0.1\n",
      "alpha =  0.87\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5709154230\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782901822\n",
      "learning_rate =  0.1\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713447409\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5787013171\n",
      "learning_rate =  0.1\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717678856\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5791065331\n",
      "learning_rate =  0.1\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721850124\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5795059792\n",
      "learning_rate =  0.1\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5725962708\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5798997992\n",
      "learning_rate =  0.1\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730018054\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5802881319\n",
      "learning_rate =  0.1\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734017559\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5806711114\n",
      "learning_rate =  0.1\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737962572\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810488673\n",
      "learning_rate =  0.1\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741854397\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814215250\n",
      "learning_rate =  0.1\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745694298\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5817892055\n",
      "learning_rate =  0.1\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749483493\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821520260\n",
      "learning_rate =  0.1\n",
      "alpha =  0.98\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5753223163\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825100999\n",
      "learning_rate =  0.1\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5756914452\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5828635368\n",
      "learning_rate =  0.1\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760558466\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832124430\n",
      "learning_rate =  0.05\n",
      "alpha =  0.0\n",
      "final training loss =  0.9060000000\n",
      "final training accuracy =  0.2211329787\n",
      "final testing loss =  0.8980000000\n",
      "final testing accuracy =  0.2480928785\n",
      "learning_rate =  0.05\n",
      "alpha =  0.01\n",
      "final training loss =  0.8600000000\n",
      "final training accuracy =  0.3647149436\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3799221558\n",
      "learning_rate =  0.05\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952629400\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086862372\n",
      "learning_rate =  0.05\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207204\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265627758\n",
      "learning_rate =  0.05\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583377\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396402912\n",
      "learning_rate =  0.05\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499703137\n",
      "learning_rate =  0.05\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099437\n",
      "learning_rate =  0.05\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869621\n",
      "learning_rate =  0.05\n",
      "alpha =  0.08\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4608863420\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721243211\n",
      "learning_rate =  0.05\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666526016\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777343361\n",
      "learning_rate =  0.05\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4718246164\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827641655\n",
      "learning_rate =  0.05\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765118284\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873200553\n",
      "learning_rate =  0.05\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807956019\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914812545\n",
      "learning_rate =  0.05\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381117\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953084749\n",
      "learning_rate =  0.05\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883879714\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988492767\n",
      "learning_rate =  0.05\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839373\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021416297\n",
      "learning_rate =  0.05\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574274\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052163458\n",
      "learning_rate =  0.05\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342819\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080987837\n",
      "learning_rate =  0.05\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360246\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108100755\n",
      "learning_rate =  0.05\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807851\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133680246\n",
      "learning_rate =  0.05\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839865\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157877770\n",
      "learning_rate =  0.05\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588656\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180823307\n",
      "learning_rate =  0.05\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202629279\n",
      "learning_rate =  0.05\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393614\n",
      "learning_rate =  0.05\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202160\n",
      "learning_rate =  0.05\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130617\n",
      "learning_rate =  0.05\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246096\n",
      "learning_rate =  0.05\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203639609\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297608379\n",
      "learning_rate =  0.05\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934362\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314270966\n",
      "learning_rate =  0.05\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558085\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330281931\n",
      "learning_rate =  0.05\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555102\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345684635\n",
      "learning_rate =  0.05\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965562\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518334\n",
      "learning_rate =  0.05\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283825946\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818681\n",
      "learning_rate =  0.05\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169510\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618156\n",
      "learning_rate =  0.05\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026654\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946437\n",
      "learning_rate =  0.05\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425241\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830709\n",
      "learning_rate =  0.05\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390877\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427295945\n",
      "learning_rate =  0.05\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947146\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365129\n",
      "learning_rate =  0.05\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115820\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059474\n",
      "learning_rate =  0.05\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398589\n",
      "learning_rate =  0.05\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369468\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400643\n",
      "learning_rate =  0.05\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490443\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082499\n",
      "learning_rate =  0.05\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296091\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459836\n",
      "learning_rate =  0.05\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547260\n",
      "learning_rate =  0.05\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358394\n",
      "learning_rate =  0.05\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905968\n",
      "learning_rate =  0.05\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201892\n",
      "learning_rate =  0.05\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257325\n",
      "learning_rate =  0.05\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082737\n",
      "learning_rate =  0.05\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.05\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082252\n",
      "learning_rate =  0.05\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.05\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.05\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.05\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716968\n",
      "learning_rate =  0.05\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.05\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.05\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.05\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.05\n",
      "alpha =  0.59\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5555227329\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5635426576\n",
      "learning_rate =  0.05\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5562208607\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642118885\n",
      "learning_rate =  0.05\n",
      "alpha =  0.61\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5569046896\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648673718\n",
      "learning_rate =  0.05\n",
      "alpha =  0.62\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5575747159\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5655095865\n",
      "learning_rate =  0.05\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582314121\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5661389889\n",
      "learning_rate =  0.05\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5588752284\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5667560134\n",
      "learning_rate =  0.05\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595065940\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5673610742\n",
      "learning_rate =  0.05\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601259183\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5679545662\n",
      "learning_rate =  0.05\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607335923\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5685368665\n",
      "learning_rate =  0.05\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613299894\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5691083352\n",
      "learning_rate =  0.05\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619154665\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5696693164\n",
      "learning_rate =  0.05\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5624903651\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702201391\n",
      "learning_rate =  0.05\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630550119\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707611182\n",
      "learning_rate =  0.05\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636097196\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712925550\n",
      "learning_rate =  0.05\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5641547880\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5718147382\n",
      "learning_rate =  0.05\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5646905041\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5723279444\n",
      "learning_rate =  0.05\n",
      "alpha =  0.75\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5652171436\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728324389\n",
      "learning_rate =  0.05\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657349707\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733284760\n",
      "learning_rate =  0.05\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662442390\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738163000\n",
      "learning_rate =  0.05\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667451923\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742961454\n",
      "learning_rate =  0.05\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672380647\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747682374\n",
      "learning_rate =  0.05\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677230811\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752327926\n",
      "learning_rate =  0.05\n",
      "alpha =  0.81\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5682004580\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5756900191\n",
      "learning_rate =  0.05\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686704035\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5761401171\n",
      "learning_rate =  0.05\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691331182\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765832795\n",
      "learning_rate =  0.05\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695887950\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5770196917\n",
      "learning_rate =  0.05\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700376197\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774495325\n",
      "learning_rate =  0.05\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704797715\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778729740\n",
      "learning_rate =  0.05\n",
      "alpha =  0.87\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5709154230\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782901822\n",
      "learning_rate =  0.05\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713447409\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5787013171\n",
      "learning_rate =  0.05\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717678856\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5791065331\n",
      "learning_rate =  0.05\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721850124\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5795059792\n",
      "learning_rate =  0.05\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5725962708\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5798997992\n",
      "learning_rate =  0.05\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730018054\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5802881319\n",
      "learning_rate =  0.05\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734017559\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5806711114\n",
      "learning_rate =  0.05\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737962572\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810488673\n",
      "learning_rate =  0.05\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741854397\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814215250\n",
      "learning_rate =  0.05\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745694298\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5817892055\n",
      "learning_rate =  0.05\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749483493\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821520260\n",
      "learning_rate =  0.05\n",
      "alpha =  0.98\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5753223163\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825100999\n",
      "learning_rate =  0.05\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5756914452\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5828635368\n",
      "learning_rate =  0.05\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760558466\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832124430\n",
      "learning_rate =  0.01\n",
      "alpha =  0.0\n",
      "final training loss =  0.8900000000\n",
      "final training accuracy =  0.2587084011\n",
      "final testing loss =  0.8720000000\n",
      "final testing accuracy =  0.2811687065\n",
      "learning_rate =  0.01\n",
      "alpha =  0.01\n",
      "final training loss =  0.8600000000\n",
      "final training accuracy =  0.3647151134\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3799118032\n",
      "learning_rate =  0.01\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952629401\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086861491\n",
      "learning_rate =  0.01\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207204\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265627748\n",
      "learning_rate =  0.01\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583377\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396402912\n",
      "learning_rate =  0.01\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499703137\n",
      "learning_rate =  0.01\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099437\n",
      "learning_rate =  0.01\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869621\n",
      "learning_rate =  0.01\n",
      "alpha =  0.08\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4608863420\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721243211\n",
      "learning_rate =  0.01\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666526016\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777343361\n",
      "learning_rate =  0.01\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4718246164\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827641655\n",
      "learning_rate =  0.01\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765118284\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873200553\n",
      "learning_rate =  0.01\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807956019\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914812545\n",
      "learning_rate =  0.01\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381117\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953084749\n",
      "learning_rate =  0.01\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883879714\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988492767\n",
      "learning_rate =  0.01\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839373\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021416297\n",
      "learning_rate =  0.01\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574274\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052163458\n",
      "learning_rate =  0.01\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342819\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080987837\n",
      "learning_rate =  0.01\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360246\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108100755\n",
      "learning_rate =  0.01\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807851\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133680246\n",
      "learning_rate =  0.01\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839865\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157877770\n",
      "learning_rate =  0.01\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588656\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180823307\n",
      "learning_rate =  0.01\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202629279\n",
      "learning_rate =  0.01\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393614\n",
      "learning_rate =  0.01\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202160\n",
      "learning_rate =  0.01\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130617\n",
      "learning_rate =  0.01\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246096\n",
      "learning_rate =  0.01\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203639609\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297608379\n",
      "learning_rate =  0.01\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934362\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314270966\n",
      "learning_rate =  0.01\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558085\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330281931\n",
      "learning_rate =  0.01\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555102\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345684635\n",
      "learning_rate =  0.01\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965562\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518334\n",
      "learning_rate =  0.01\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283825946\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818681\n",
      "learning_rate =  0.01\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169510\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618156\n",
      "learning_rate =  0.01\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026654\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946437\n",
      "learning_rate =  0.01\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425241\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830709\n",
      "learning_rate =  0.01\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390877\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427295945\n",
      "learning_rate =  0.01\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947146\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365129\n",
      "learning_rate =  0.01\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115820\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059474\n",
      "learning_rate =  0.01\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398589\n",
      "learning_rate =  0.01\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369468\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400643\n",
      "learning_rate =  0.01\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490443\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082499\n",
      "learning_rate =  0.01\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296091\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459836\n",
      "learning_rate =  0.01\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547260\n",
      "learning_rate =  0.01\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358394\n",
      "learning_rate =  0.01\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905968\n",
      "learning_rate =  0.01\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201892\n",
      "learning_rate =  0.01\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257325\n",
      "learning_rate =  0.01\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082737\n",
      "learning_rate =  0.01\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.01\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082252\n",
      "learning_rate =  0.01\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.01\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.01\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.01\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716968\n",
      "learning_rate =  0.01\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.01\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.01\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.01\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.01\n",
      "alpha =  0.59\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5555227329\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5635426576\n",
      "learning_rate =  0.01\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5562208607\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642118885\n",
      "learning_rate =  0.01\n",
      "alpha =  0.61\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5569046896\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648673718\n",
      "learning_rate =  0.01\n",
      "alpha =  0.62\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5575747159\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5655095865\n",
      "learning_rate =  0.01\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582314121\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5661389889\n",
      "learning_rate =  0.01\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5588752284\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5667560134\n",
      "learning_rate =  0.01\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595065940\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5673610742\n",
      "learning_rate =  0.01\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601259183\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5679545662\n",
      "learning_rate =  0.01\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607335923\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5685368665\n",
      "learning_rate =  0.01\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613299894\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5691083352\n",
      "learning_rate =  0.01\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619154665\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5696693164\n",
      "learning_rate =  0.01\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5624903651\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702201391\n",
      "learning_rate =  0.01\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630550119\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707611182\n",
      "learning_rate =  0.01\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636097196\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712925550\n",
      "learning_rate =  0.01\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5641547880\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5718147382\n",
      "learning_rate =  0.01\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5646905041\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5723279444\n",
      "learning_rate =  0.01\n",
      "alpha =  0.75\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5652171436\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728324389\n",
      "learning_rate =  0.01\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657349707\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733284760\n",
      "learning_rate =  0.01\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662442390\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738163000\n",
      "learning_rate =  0.01\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667451923\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742961454\n",
      "learning_rate =  0.01\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672380647\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747682374\n",
      "learning_rate =  0.01\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677230811\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752327926\n",
      "learning_rate =  0.01\n",
      "alpha =  0.81\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5682004580\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5756900191\n",
      "learning_rate =  0.01\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686704035\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5761401171\n",
      "learning_rate =  0.01\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691331182\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765832795\n",
      "learning_rate =  0.01\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695887950\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5770196917\n",
      "learning_rate =  0.01\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700376197\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774495325\n",
      "learning_rate =  0.01\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704797715\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778729740\n",
      "learning_rate =  0.01\n",
      "alpha =  0.87\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5709154230\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782901822\n",
      "learning_rate =  0.01\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713447409\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5787013171\n",
      "learning_rate =  0.01\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717678856\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5791065331\n",
      "learning_rate =  0.01\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721850124\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5795059792\n",
      "learning_rate =  0.01\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5725962708\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5798997992\n",
      "learning_rate =  0.01\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730018054\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5802881319\n",
      "learning_rate =  0.01\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734017559\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5806711114\n",
      "learning_rate =  0.01\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737962572\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810488673\n",
      "learning_rate =  0.01\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741854397\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814215250\n",
      "learning_rate =  0.01\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745694298\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5817892055\n",
      "learning_rate =  0.01\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749483493\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821520260\n",
      "learning_rate =  0.01\n",
      "alpha =  0.98\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5753223163\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825100999\n",
      "learning_rate =  0.01\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5756914452\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5828635368\n",
      "learning_rate =  0.01\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760558466\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832124430\n",
      "learning_rate =  0.005\n",
      "alpha =  0.0\n",
      "final training loss =  0.8860000000\n",
      "final training accuracy =  0.2807231511\n",
      "final testing loss =  0.8700000000\n",
      "final testing accuracy =  0.3000054486\n",
      "learning_rate =  0.005\n",
      "alpha =  0.01\n",
      "final training loss =  0.8580000000\n",
      "final training accuracy =  0.3647778913\n",
      "final testing loss =  0.8540000000\n",
      "final testing accuracy =  0.3797671827\n",
      "learning_rate =  0.005\n",
      "alpha =  0.02\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3952634158\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.4086698071\n",
      "learning_rate =  0.005\n",
      "alpha =  0.03\n",
      "final training loss =  0.8220000000\n",
      "final training accuracy =  0.4139207254\n",
      "final testing loss =  0.8360000000\n",
      "final testing accuracy =  0.4265611845\n",
      "learning_rate =  0.005\n",
      "alpha =  0.04\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4274583378\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.4396401255\n",
      "learning_rate =  0.005\n",
      "alpha =  0.05\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4381078013\n",
      "final testing loss =  0.8180000000\n",
      "final testing accuracy =  0.4499702955\n",
      "learning_rate =  0.005\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4468935357\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4585099416\n",
      "learning_rate =  0.005\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543736039\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657869619\n",
      "learning_rate =  0.005\n",
      "alpha =  0.08\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4608863420\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721243211\n",
      "learning_rate =  0.005\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666526016\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777343361\n",
      "learning_rate =  0.005\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4718246164\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827641655\n",
      "learning_rate =  0.005\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765118284\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873200553\n",
      "learning_rate =  0.005\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807956019\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914812545\n",
      "learning_rate =  0.005\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381117\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953084749\n",
      "learning_rate =  0.005\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883879714\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988492767\n",
      "learning_rate =  0.005\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839373\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021416297\n",
      "learning_rate =  0.005\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574274\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052163458\n",
      "learning_rate =  0.005\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342819\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080987837\n",
      "learning_rate =  0.005\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360246\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108100755\n",
      "learning_rate =  0.005\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807851\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133680246\n",
      "learning_rate =  0.005\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839865\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157877770\n",
      "learning_rate =  0.005\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588656\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180823307\n",
      "learning_rate =  0.005\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202629279\n",
      "learning_rate =  0.005\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393614\n",
      "learning_rate =  0.005\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202160\n",
      "learning_rate =  0.005\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130617\n",
      "learning_rate =  0.005\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246096\n",
      "learning_rate =  0.005\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203639609\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297608379\n",
      "learning_rate =  0.005\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934362\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314270966\n",
      "learning_rate =  0.005\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558085\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330281931\n",
      "learning_rate =  0.005\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555102\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345684635\n",
      "learning_rate =  0.005\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965562\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518334\n",
      "learning_rate =  0.005\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283825946\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818681\n",
      "learning_rate =  0.005\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169510\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618156\n",
      "learning_rate =  0.005\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026654\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946437\n",
      "learning_rate =  0.005\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425241\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830709\n",
      "learning_rate =  0.005\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390877\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427295945\n",
      "learning_rate =  0.005\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947146\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365129\n",
      "learning_rate =  0.005\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115820\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059474\n",
      "learning_rate =  0.005\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398589\n",
      "learning_rate =  0.005\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369468\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400643\n",
      "learning_rate =  0.005\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490443\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082499\n",
      "learning_rate =  0.005\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296091\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459836\n",
      "learning_rate =  0.005\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547260\n",
      "learning_rate =  0.005\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358394\n",
      "learning_rate =  0.005\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905968\n",
      "learning_rate =  0.005\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201892\n",
      "learning_rate =  0.005\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257325\n",
      "learning_rate =  0.005\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082737\n",
      "learning_rate =  0.005\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.005\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082252\n",
      "learning_rate =  0.005\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.005\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.005\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.005\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716968\n",
      "learning_rate =  0.005\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.005\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.005\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.005\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.005\n",
      "alpha =  0.59\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5555227329\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5635426576\n",
      "learning_rate =  0.005\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5562208607\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642118885\n",
      "learning_rate =  0.005\n",
      "alpha =  0.61\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5569046896\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648673718\n",
      "learning_rate =  0.005\n",
      "alpha =  0.62\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5575747159\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5655095865\n",
      "learning_rate =  0.005\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582314121\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5661389889\n",
      "learning_rate =  0.005\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5588752284\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5667560134\n",
      "learning_rate =  0.005\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595065940\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5673610742\n",
      "learning_rate =  0.005\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601259183\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5679545662\n",
      "learning_rate =  0.005\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607335923\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5685368665\n",
      "learning_rate =  0.005\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613299894\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5691083352\n",
      "learning_rate =  0.005\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619154665\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5696693164\n",
      "learning_rate =  0.005\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5624903651\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702201391\n",
      "learning_rate =  0.005\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630550119\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707611182\n",
      "learning_rate =  0.005\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636097196\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712925550\n",
      "learning_rate =  0.005\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5641547880\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5718147382\n",
      "learning_rate =  0.005\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5646905041\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5723279444\n",
      "learning_rate =  0.005\n",
      "alpha =  0.75\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5652171436\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728324389\n",
      "learning_rate =  0.005\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657349707\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733284760\n",
      "learning_rate =  0.005\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662442390\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738163000\n",
      "learning_rate =  0.005\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667451923\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742961454\n",
      "learning_rate =  0.005\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672380647\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747682374\n",
      "learning_rate =  0.005\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677230811\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752327926\n",
      "learning_rate =  0.005\n",
      "alpha =  0.81\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5682004580\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5756900191\n",
      "learning_rate =  0.005\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686704035\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5761401171\n",
      "learning_rate =  0.005\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691331182\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765832795\n",
      "learning_rate =  0.005\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695887950\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5770196917\n",
      "learning_rate =  0.005\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700376197\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774495325\n",
      "learning_rate =  0.005\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704797715\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778729740\n",
      "learning_rate =  0.005\n",
      "alpha =  0.87\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5709154230\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782901822\n",
      "learning_rate =  0.005\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713447409\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5787013171\n",
      "learning_rate =  0.005\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717678856\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5791065331\n",
      "learning_rate =  0.005\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721850124\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5795059792\n",
      "learning_rate =  0.005\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5725962708\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5798997992\n",
      "learning_rate =  0.005\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730018054\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5802881319\n",
      "learning_rate =  0.005\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734017559\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5806711114\n",
      "learning_rate =  0.005\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737962572\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810488673\n",
      "learning_rate =  0.005\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741854397\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814215250\n",
      "learning_rate =  0.005\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745694298\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5817892055\n",
      "learning_rate =  0.005\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749483493\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821520260\n",
      "learning_rate =  0.005\n",
      "alpha =  0.98\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5753223163\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825100999\n",
      "learning_rate =  0.005\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5756914452\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5828635368\n",
      "learning_rate =  0.005\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760558466\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832124430\n",
      "learning_rate =  0.001\n",
      "alpha =  0.0\n",
      "final training loss =  0.8360000000\n",
      "final training accuracy =  0.3429036241\n",
      "final testing loss =  0.8440000000\n",
      "final testing accuracy =  0.3563028117\n",
      "learning_rate =  0.001\n",
      "alpha =  0.01\n",
      "final training loss =  0.8300000000\n",
      "final training accuracy =  0.3755414382\n",
      "final testing loss =  0.8380000000\n",
      "final testing accuracy =  0.3884508917\n",
      "learning_rate =  0.001\n",
      "alpha =  0.02\n",
      "final training loss =  0.8200000000\n",
      "final training accuracy =  0.3981689332\n",
      "final testing loss =  0.8320000000\n",
      "final testing accuracy =  0.4106991155\n",
      "learning_rate =  0.001\n",
      "alpha =  0.03\n",
      "final training loss =  0.8080000000\n",
      "final training accuracy =  0.4148463639\n",
      "final testing loss =  0.8280000000\n",
      "final testing accuracy =  0.4270699225\n",
      "learning_rate =  0.001\n",
      "alpha =  0.04\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4277785896\n",
      "final testing loss =  0.8200000000\n",
      "final testing accuracy =  0.4397453310\n",
      "learning_rate =  0.001\n",
      "alpha =  0.05\n",
      "final training loss =  0.7960000000\n",
      "final training accuracy =  0.4382242796\n",
      "final testing loss =  0.8140000000\n",
      "final testing accuracy =  0.4499696636\n",
      "learning_rate =  0.001\n",
      "alpha =  0.06\n",
      "final training loss =  0.7940000000\n",
      "final training accuracy =  0.4469373561\n",
      "final testing loss =  0.8120000000\n",
      "final testing accuracy =  0.4584875202\n",
      "learning_rate =  0.001\n",
      "alpha =  0.07\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4543904981\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4657653358\n",
      "learning_rate =  0.001\n",
      "alpha =  0.08\n",
      "final training loss =  0.7900000000\n",
      "final training accuracy =  0.4608929774\n",
      "final testing loss =  0.8040000000\n",
      "final testing accuracy =  0.4721080830\n",
      "learning_rate =  0.001\n",
      "alpha =  0.09\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4666552460\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4777231755\n",
      "learning_rate =  0.001\n",
      "alpha =  0.1\n",
      "final training loss =  0.7900000000\n",
      "final training accuracy =  0.4718256827\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4827567916\n",
      "learning_rate =  0.001\n",
      "alpha =  0.11\n",
      "final training loss =  0.7860000000\n",
      "final training accuracy =  0.4765122626\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873152751\n",
      "learning_rate =  0.001\n",
      "alpha =  0.12\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4807957801\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4914781835\n",
      "learning_rate =  0.001\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847381853\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953065095\n",
      "learning_rate =  0.001\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4883880020\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4988480200\n",
      "learning_rate =  0.001\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917839501\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5021408257\n",
      "learning_rate =  0.001\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949574328\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052158306\n",
      "learning_rate =  0.001\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979342842\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5080984531\n",
      "learning_rate =  0.001\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007360255\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108098629\n",
      "learning_rate =  0.001\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033807855\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133678876\n",
      "learning_rate =  0.001\n",
      "alpha =  0.2\n",
      "final training loss =  0.7760000000\n",
      "final training accuracy =  0.5058839867\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157876885\n",
      "learning_rate =  0.001\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082588657\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180822734\n",
      "learning_rate =  0.001\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105168735\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5202628908\n",
      "learning_rate =  0.001\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126679878\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223393373\n",
      "learning_rate =  0.001\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147209584\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243202003\n",
      "learning_rate =  0.001\n",
      "alpha =  0.25\n",
      "final training loss =  0.7680000000\n",
      "final training accuracy =  0.5166835042\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262130516\n",
      "learning_rate =  0.001\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185624712\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280246029\n",
      "learning_rate =  0.001\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203639609\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297608336\n",
      "learning_rate =  0.001\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934362\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314270938\n",
      "learning_rate =  0.001\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558085\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330281912\n",
      "learning_rate =  0.001\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555102\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345684623\n",
      "learning_rate =  0.001\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965562\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518327\n",
      "learning_rate =  0.001\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283825946\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818676\n",
      "learning_rate =  0.001\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169510\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618153\n",
      "learning_rate =  0.001\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026654\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946435\n",
      "learning_rate =  0.001\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425241\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830708\n",
      "learning_rate =  0.001\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390877\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427295944\n",
      "learning_rate =  0.001\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947146\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365129\n",
      "learning_rate =  0.001\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115820\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059474\n",
      "learning_rate =  0.001\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398589\n",
      "learning_rate =  0.001\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369468\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400643\n",
      "learning_rate =  0.001\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490443\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082499\n",
      "learning_rate =  0.001\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296091\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459836\n",
      "learning_rate =  0.001\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547260\n",
      "learning_rate =  0.001\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358394\n",
      "learning_rate =  0.001\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905968\n",
      "learning_rate =  0.001\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201892\n",
      "learning_rate =  0.001\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257325\n",
      "learning_rate =  0.001\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082737\n",
      "learning_rate =  0.001\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.001\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082252\n",
      "learning_rate =  0.001\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.001\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.001\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.001\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716968\n",
      "learning_rate =  0.001\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.001\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.001\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.001\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.001\n",
      "alpha =  0.59\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5555227329\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5635426576\n",
      "learning_rate =  0.001\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5562208607\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642118885\n",
      "learning_rate =  0.001\n",
      "alpha =  0.61\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5569046896\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648673718\n",
      "learning_rate =  0.001\n",
      "alpha =  0.62\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5575747159\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5655095865\n",
      "learning_rate =  0.001\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582314121\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5661389889\n",
      "learning_rate =  0.001\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5588752284\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5667560134\n",
      "learning_rate =  0.001\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595065940\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5673610742\n",
      "learning_rate =  0.001\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601259183\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5679545662\n",
      "learning_rate =  0.001\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607335923\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5685368665\n",
      "learning_rate =  0.001\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613299894\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5691083352\n",
      "learning_rate =  0.001\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619154665\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5696693164\n",
      "learning_rate =  0.001\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5624903651\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702201391\n",
      "learning_rate =  0.001\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630550119\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707611182\n",
      "learning_rate =  0.001\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636097196\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712925550\n",
      "learning_rate =  0.001\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5641547880\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5718147382\n",
      "learning_rate =  0.001\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5646905041\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5723279444\n",
      "learning_rate =  0.001\n",
      "alpha =  0.75\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5652171436\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728324389\n",
      "learning_rate =  0.001\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657349707\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733284760\n",
      "learning_rate =  0.001\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662442390\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738163000\n",
      "learning_rate =  0.001\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667451923\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742961454\n",
      "learning_rate =  0.001\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672380647\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747682374\n",
      "learning_rate =  0.001\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677230811\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752327926\n",
      "learning_rate =  0.001\n",
      "alpha =  0.81\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5682004580\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5756900191\n",
      "learning_rate =  0.001\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686704035\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5761401171\n",
      "learning_rate =  0.001\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691331182\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765832795\n",
      "learning_rate =  0.001\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695887950\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5770196917\n",
      "learning_rate =  0.001\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700376197\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774495325\n",
      "learning_rate =  0.001\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704797715\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778729740\n",
      "learning_rate =  0.001\n",
      "alpha =  0.87\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5709154230\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782901822\n",
      "learning_rate =  0.001\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713447409\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5787013171\n",
      "learning_rate =  0.001\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717678856\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5791065331\n",
      "learning_rate =  0.001\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721850124\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5795059792\n",
      "learning_rate =  0.001\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5725962708\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5798997992\n",
      "learning_rate =  0.001\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730018054\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5802881319\n",
      "learning_rate =  0.001\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734017559\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5806711114\n",
      "learning_rate =  0.001\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737962572\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810488673\n",
      "learning_rate =  0.001\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741854397\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814215250\n",
      "learning_rate =  0.001\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745694298\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5817892055\n",
      "learning_rate =  0.001\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749483493\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821520260\n",
      "learning_rate =  0.001\n",
      "alpha =  0.98\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5753223163\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825100999\n",
      "learning_rate =  0.001\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5756914452\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5828635368\n",
      "learning_rate =  0.001\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760558466\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832124430\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.0\n",
      "final training loss =  0.8000000000\n",
      "final training accuracy =  0.3734625244\n",
      "final testing loss =  0.8240000000\n",
      "final testing accuracy =  0.3865314557\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.01\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.3920603436\n",
      "final testing loss =  0.8220000000\n",
      "final testing accuracy =  0.4047801660\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.02\n",
      "final training loss =  0.7960000000\n",
      "final training accuracy =  0.4072943735\n",
      "final testing loss =  0.8160000000\n",
      "final testing accuracy =  0.4197154860\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.03\n",
      "final training loss =  0.7960000000\n",
      "final training accuracy =  0.4199833000\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4321445189\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.04\n",
      "final training loss =  0.7980000000\n",
      "final training accuracy =  0.4307180845\n",
      "final testing loss =  0.8100000000\n",
      "final testing accuracy =  0.4426497713\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.05\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4399305994\n",
      "final testing loss =  0.8060000000\n",
      "final testing accuracy =  0.4516568751\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.06\n",
      "final training loss =  0.7920000000\n",
      "final training accuracy =  0.4479403495\n",
      "final testing loss =  0.8020000000\n",
      "final testing accuracy =  0.4594806766\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.07\n",
      "final training loss =  0.7900000000\n",
      "final training accuracy =  0.4549866028\n",
      "final testing loss =  0.8000000000\n",
      "final testing accuracy =  0.4663569248\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.08\n",
      "final training loss =  0.7900000000\n",
      "final training accuracy =  0.4612507106\n",
      "final testing loss =  0.7980000000\n",
      "final testing accuracy =  0.4724642824\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.09\n",
      "final training loss =  0.7900000000\n",
      "final training accuracy =  0.4668717715\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4779397689\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.1\n",
      "final training loss =  0.7880000000\n",
      "final training accuracy =  0.4719577327\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4828897051\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.11\n",
      "final training loss =  0.7840000000\n",
      "final training accuracy =  0.4765933344\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4873975437\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.12\n",
      "final training loss =  0.7800000000\n",
      "final training accuracy =  0.4808458500\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.4915295267\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.13\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4847692720\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4953388108\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.14\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4884073938\n",
      "final testing loss =  0.7960000000\n",
      "final testing accuracy =  0.4988685039\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.15\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4917960978\n",
      "final testing loss =  0.7940000000\n",
      "final testing accuracy =  0.5021539200\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.16\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4949650713\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5052242694\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.17\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.4979391038\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5081039372\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.18\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5007390759\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5108134577\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.19\n",
      "final training loss =  0.7780000000\n",
      "final training accuracy =  0.5033827216\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5133702650\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.2\n",
      "final training loss =  0.7740000000\n",
      "final training accuracy =  0.5058852186\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5157892753\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.21\n",
      "final training loss =  0.7720000000\n",
      "final training accuracy =  0.5082596514\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5180833425\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.22\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5105173758\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5202636179\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.23\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5126683094\n",
      "final testing loss =  0.7920000000\n",
      "final testing accuracy =  0.5223398365\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.24\n",
      "final training loss =  0.7700000000\n",
      "final training accuracy =  0.5147211648\n",
      "final testing loss =  0.7900000000\n",
      "final testing accuracy =  0.5243205463\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.25\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.5166836369\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5262132935\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.26\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5185625566\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5280247736\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.27\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5203640159\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5297609549\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.28\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5220934717\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5314271807\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.29\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5237558314\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5330282538\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5253555251\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5345685077\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.31\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5268965658\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5360518657\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.32\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5283826009\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5374818918\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.33\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5298169551\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5388618330\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.34\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5312026680\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5401946565\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5325425258\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5414830804\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.36\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5338390888\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5427296014\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.37\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5350947153\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5439365181\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.38\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.5363115825\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5451059512\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.39\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5374917041\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5462398617\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.4\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5386369470\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5473400663\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5397490445\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5484082513\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5408296092\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5494459847\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.43\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5418801436\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5504547267\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.44\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5429020502\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5514358399\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.45\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5438966398\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5523905972\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.46\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5448651393\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5533201894\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5458086988\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5542257326\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5467283977\n",
      "final testing loss =  0.7880000000\n",
      "final testing accuracy =  0.5551082738\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.49\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5476252505\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5559687963\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5485002114\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568082253\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.51\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5493541795\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5576274316\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.52\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5501880024\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5584272362\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.53\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5510024804\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592084140\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5517983695\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5599716967\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.55\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5525763849\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607177764\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.56\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5533372038\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614473078\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5540814676\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621609113\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.58\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5548097848\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5628591752\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.59\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5555227329\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5635426576\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.6\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5562208607\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642118885\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.61\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5569046896\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648673718\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.62\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5575747159\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5655095865\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582314121\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5661389889\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5588752284\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5667560134\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595065940\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5673610742\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601259183\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5679545662\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607335923\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5685368665\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613299894\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5691083352\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619154665\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5696693164\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5624903651\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702201391\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630550119\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707611182\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636097196\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712925550\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.73\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5641547880\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5718147382\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.74\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5646905041\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5723279444\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.75\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5652171436\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728324389\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657349707\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733284760\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662442390\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738163000\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667451923\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742961454\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672380647\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747682374\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677230811\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752327926\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.81\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5682004580\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5756900191\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686704035\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5761401171\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691331182\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765832795\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695887950\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5770196917\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700376197\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774495325\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704797715\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778729740\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.87\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5709154230\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782901822\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713447409\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5787013171\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717678856\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5791065331\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721850124\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5795059792\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5725962708\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5798997992\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730018054\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5802881319\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734017559\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5806711114\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737962572\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810488673\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741854397\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814215250\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745694298\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5817892055\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749483493\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821520260\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.98\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5753223163\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825100999\n",
      "learning_rate =  0.0005\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5756914452\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5828635368\n",
      "learning_rate =  0.0005\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760558466\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832124430\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.0\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.4502127802\n",
      "final testing loss =  0.7740000000\n",
      "final testing accuracy =  0.4610549139\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.01\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.4545124310\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.4652698187\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.02\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4586237670\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.4692983914\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.03\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4625572779\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.4731510637\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.04\n",
      "final training loss =  0.7660000000\n",
      "final training accuracy =  0.4663227892\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.4768376049\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.05\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4699295075\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.4803671674\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.06\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4733860635\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.4837483297\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.07\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4767005513\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.4869891354\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.08\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4798805649\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.4900971295\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.09\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4829332321\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.4930793927\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.1\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4858652460\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.4959425722\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.11\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4886828940\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.4986929113\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.12\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4913920849\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5013362760\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.13\n",
      "final training loss =  0.7640000000\n",
      "final training accuracy =  0.4939983739\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5038781801\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.14\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.4965069862\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5063238087\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.15\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.4989228384\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5086780396\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.16\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5012505587\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5109454634\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.17\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5034945057\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5131304024\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.18\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5056587856\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5152369275\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.19\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5077472687\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5172688748\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.2\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5097636043\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5192298603\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.21\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5117112344\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5211232943\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.22\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5135934074\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5229523939\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.23\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5154131899\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5247201956\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.24\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5171734781\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5264295663\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.25\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5188770084\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5280832141\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.26\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5205263671\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5296836978\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.27\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5221239998\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5312334364\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.28\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5236722202\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5327347177\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.29\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5251732175\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5341897058\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.3\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5266290643\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5356004491\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.31\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5280417239\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5369688870\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.32\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5294130559\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5382968565\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.33\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5307448235\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5395860984\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.34\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5320386981\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5408382626\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5332962655\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5420549139\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.36\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5345190305\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5432375368\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.37\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5357084216\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5443875400\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.38\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5368657954\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5455062611\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.39\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5379924410\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5465949704\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.4\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5390895834\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5476548749\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5401583874\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5486871220\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.42\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5411999610\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5496928024\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.43\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5422153583\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5506729539\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.44\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5432055830\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5516285639\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.45\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5441715906\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5525605725\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.46\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5451142915\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5534698748\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5460345532\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5543573236\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.48\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5469332029\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5552237317\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.49\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5478110292\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5560698739\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.5\n",
      "final training loss =  0.7620000000\n",
      "final training accuracy =  0.5486687846\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5568964891\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.51\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5495071874\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5577042825\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.52\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5503269232\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5584939269\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.53\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5511286470\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5592660647\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.54\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5519129844\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5600213098\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.55\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5526805334\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5607602482\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.56\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5534318660\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5614834406\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5541675290\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5621914226\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.58\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5548880457\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5628847067\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.59\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5555939170\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5635637835\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.6\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5562856225\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5642291221\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.61\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5569636215\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5648811722\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.62\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5576283542\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5655203640\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.63\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5582802423\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5661471101\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.64\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5589196905\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5667618059\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.65\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5595470865\n",
      "final testing loss =  0.7760000000\n",
      "final testing accuracy =  0.5673648303\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.66\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5601628027\n",
      "final testing loss =  0.7740000000\n",
      "final testing accuracy =  0.5679565469\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.67\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5607671963\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5685373046\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.68\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5613606103\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5691074380\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5619433740\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5696672688\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5625158039\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5702171055\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5630782042\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5707572447\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5636308672\n",
      "final testing loss =  0.7720000000\n",
      "final testing accuracy =  0.5712879715\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.73\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5641740742\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5718095598\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.74\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5647080955\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5723222732\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.75\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5652331915\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5728263650\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.76\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5657496126\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5733220793\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.77\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5662576001\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5738096508\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.78\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5667573862\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5742893055\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.79\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5672491949\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5747612611\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.8\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5677332417\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5752257273\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.81\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5682097347\n",
      "final testing loss =  0.7640000000\n",
      "final testing accuracy =  0.5756829064\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5686788744\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5761329932\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5691408541\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5765761755\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.84\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5695958605\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5770126346\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.85\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5700440738\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5774425455\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.86\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5704856677\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5778660768\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.87\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5709208101\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5782833916\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.88\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5713496634\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5786946471\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.89\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5717723841\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5790999954\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.9\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5721891238\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5794995833\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.91\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5726000289\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5798935527\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.92\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5730052408\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5802820408\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.93\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5734048967\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5806651803\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5737991289\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5810430996\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5741880657\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5814159227\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.96\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5745718312\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5817837699\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.97\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5749505456\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5821467574\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.98\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5753243252\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5825049978\n",
      "learning_rate =  0.0001\n",
      "alpha =  0.99\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5756932829\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5828586002\n",
      "learning_rate =  0.0001\n",
      "alpha =  1.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5760575277\n",
      "final testing loss =  0.7540000000\n",
      "final testing accuracy =  0.5832076703\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.0\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.4911247355\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5001033627\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.01\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.4930616647\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5020106412\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.02\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.4949539595\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5038735598\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.03\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.4968028913\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5056933879\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.04\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.4986096912\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5074713548\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.05\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5003755514\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5092086507\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.06\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5021016269\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5109064287\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.07\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5037890363\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5125658053\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.08\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5054388631\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5141878621\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.09\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5070521572\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5157736470\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.1\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5086299356\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5173241748\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.11\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5101731841\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5188404291\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.12\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5116828574\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5203233627\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.13\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5131598812\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5217738989\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.14\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5146051523\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5231929326\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.15\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5160195401\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5245813309\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.16\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5174038872\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5259399343\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.17\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5187590106\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5272695575\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.18\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5200857021\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5285709902\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.19\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5213847296\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5298449980\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.2\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5226568376\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5310923231\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.21\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5239027479\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5323136853\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.22\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5251231606\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5335097824\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.23\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5263187548\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5346812911\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.24\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5274901889\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5358288678\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.25\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5286381017\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5369531490\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.26\n",
      "final training loss =  0.7600000000\n",
      "final training accuracy =  0.5297631129\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5380547522\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.27\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5308658237\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5391342762\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.28\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5319468172\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5401923022\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.29\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5330066595\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5412293937\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.3\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5340458997\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5422460979\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.31\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5350650707\n",
      "final testing loss =  0.7860000000\n",
      "final testing accuracy =  0.5432429454\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.32\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5360646898\n",
      "final testing loss =  0.7840000000\n",
      "final testing accuracy =  0.5442204513\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.33\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5370452589\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5451791153\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.34\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5380072654\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5461194226\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7520000000\n",
      "final training accuracy =  0.5389511823\n",
      "final testing loss =  0.7820000000\n",
      "final testing accuracy =  0.5470418441\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.36\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5398774688\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5479468368\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.37\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5407865708\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5488348445\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.38\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5416789213\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5497062981\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.39\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5425549406\n",
      "final testing loss =  0.7800000000\n",
      "final testing accuracy =  0.5505616157\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.4\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5434150371\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5514012035\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5442596072\n",
      "final testing loss =  0.7780000000\n",
      "final testing accuracy =  0.5522254560\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.42\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5450890362\n",
      "final testing loss =  0.7740000000\n",
      "final testing accuracy =  0.5530347563\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.43\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5459036980\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5538294764\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.44\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5467039562\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5546099776\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.45\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5474901639\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5553766110\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.46\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5482626640\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5561297176\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5490217900\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5568696288\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.48\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5497678656\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5575966665\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.49\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5505012059\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5583111435\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.5\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5512221166\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5590133637\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.51\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5519308952\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5597036228\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.52\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5526278308\n",
      "final testing loss =  0.7700000000\n",
      "final testing accuracy =  0.5603822077\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.53\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5533132043\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5610493976\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.54\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5539872891\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5617054639\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.55\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5546503508\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5623506704\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.56\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5553026477\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5629852734\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5559444310\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5636095224\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.58\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5565759450\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5642236598\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.59\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5571974273\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5648279215\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.6\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5578091092\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5654225367\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.61\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5584112154\n",
      "final testing loss =  0.7680000000\n",
      "final testing accuracy =  0.5660077287\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.62\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5590039646\n",
      "final testing loss =  0.7660000000\n",
      "final testing accuracy =  0.5665837143\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.63\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5595875698\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5671507047\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.64\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5601622379\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5677089051\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.65\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5607281706\n",
      "final testing loss =  0.7620000000\n",
      "final testing accuracy =  0.5682585155\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.66\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5612855638\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5687997302\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.67\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5618346085\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5693327385\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.68\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5623754905\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5698577244\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5629083905\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5703748672\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7580000000\n",
      "final training accuracy =  0.5634334847\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5708843412\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.71\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5639509444\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5713863164\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.72\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5644609365\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5718809579\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.73\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5649636235\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5723684268\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.74\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5654591636\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5728488797\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.75\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5659477111\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5733224692\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.76\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5664294159\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5737893440\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.77\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5669044243\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5742496487\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.78\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5673728788\n",
      "final testing loss =  0.7600000000\n",
      "final testing accuracy =  0.5747035243\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.79\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5678349181\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5751511080\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.8\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5682906774\n",
      "final testing loss =  0.7580000000\n",
      "final testing accuracy =  0.5755925338\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.81\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5687402884\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5760279318\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5691838796\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5764574291\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7540000000\n",
      "final training accuracy =  0.5696215760\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5768811493\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.84\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5700534996\n",
      "final testing loss =  0.7560000000\n",
      "final testing accuracy =  0.5772992130\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.85\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5704797693\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5777117377\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.86\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5709005008\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5781188380\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.87\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5713158073\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5785206254\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.88\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5717257987\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.5789172088\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.89\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5721305825\n",
      "final testing loss =  0.7480000000\n",
      "final testing accuracy =  0.5793086942\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.9\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5725302633\n",
      "final testing loss =  0.7500000000\n",
      "final testing accuracy =  0.5796951851\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.91\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5729249434\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5800767823\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.92\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5733147222\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5804535841\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.93\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5736996970\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5808256865\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5740799624\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5811931830\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5744556110\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5815561648\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.96\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5748267328\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5819147208\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.97\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5751934160\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5822689380\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.98\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5755557463\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5826189010\n",
      "learning_rate =  5e-05\n",
      "alpha =  0.99\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5759138076\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5829646925\n",
      "learning_rate =  5e-05\n",
      "alpha =  1.0\n",
      "final training loss =  0.7560000000\n",
      "final training accuracy =  0.5762676816\n",
      "final testing loss =  0.7520000000\n",
      "final testing accuracy =  0.5833063931\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.0\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5687674350\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.5750599467\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.01\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5690114049\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.5753013203\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.02\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5692542745\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.5755415868\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.03\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5694960505\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.5757807530\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.04\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5697367395\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.5760188256\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.05\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5699763481\n",
      "final testing loss =  0.7320000000\n",
      "final testing accuracy =  0.5762558113\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.06\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5702148829\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5764917167\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.07\n",
      "final training loss =  0.7340000000\n",
      "final training accuracy =  0.5704523504\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5767265483\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.08\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5706887570\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5769603128\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.09\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5709241092\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5771930166\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.1\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5711584133\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5774246661\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.11\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5713916757\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5776552678\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.12\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5716239026\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5778848279\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.13\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5718551002\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5781133529\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.14\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5720852749\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5783408490\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.15\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5723144326\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5785673223\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.16\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5725425796\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5787927791\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.17\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5727697218\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5790172256\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.18\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5729958654\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5792406677\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.19\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5732210163\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5794631116\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.2\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5734451803\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5796845632\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.21\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5736683636\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5799050285\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.22\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5738905718\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5801245135\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.23\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5741118108\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5803430241\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.24\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5743320864\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5805605660\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.25\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5745514043\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5807771451\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.26\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5747697702\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5809927671\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.27\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5749871898\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5812074378\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.28\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5752036687\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5814211628\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.29\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5754192125\n",
      "final testing loss =  0.7300000000\n",
      "final testing accuracy =  0.5816339479\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.3\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5756338267\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5818457985\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.31\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5758475168\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5820567203\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.32\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5760602883\n",
      "final testing loss =  0.7280000000\n",
      "final testing accuracy =  0.5822667187\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.33\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5762721466\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5824757994\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.34\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5764830971\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5826839677\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.35000000000000003\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5766931452\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5828912290\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.36\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5769022961\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5830975887\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.37\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5771105552\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5833030523\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.38\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5773179277\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5835076248\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.39\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5775244187\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5837113117\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.4\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5777300336\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5839141182\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.41000000000000003\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5779347774\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5841160495\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.42\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5781386552\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5843171106\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.43\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5783416720\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5845173068\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.44\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5785438330\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5847166432\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.45\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5787451431\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5849151247\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.46\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5789456073\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5851127564\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.47000000000000003\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5791452305\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5853095434\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.48\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5793440176\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5855054904\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.49\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5795419734\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5857006026\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.5\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5797391028\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5858948847\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.51\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5799354107\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5860883416\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.52\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5801309017\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5862809781\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.53\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5803255806\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5864727990\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.54\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5805194520\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5866638091\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.55\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5807125208\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5868540131\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.56\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5809047914\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5870434156\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.5700000000000001\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5810962685\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5872320214\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.58\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5812869568\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5874198350\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.59\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5814768606\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5876068610\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.6\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5816659845\n",
      "final testing loss =  0.7240000000\n",
      "final testing accuracy =  0.5877931041\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.61\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5818543331\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5879785687\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.62\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5820419107\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5881632593\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.63\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5822287219\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5883471805\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.64\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5824147709\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5885303366\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.65\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5826000621\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5887127321\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.66\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5827845999\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5888943713\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.67\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5829683886\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5890752587\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.68\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5831514325\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5892553984\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.6900000000000001\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5833337358\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5894347949\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.7000000000000001\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5835153026\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5896134524\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.71\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5836961373\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5897913750\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.72\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5838762440\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5899685671\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.73\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5840556267\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5901450328\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.74\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5842342897\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5903207763\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.75\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5844122370\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5904958017\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.76\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5845894726\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5906701130\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.77\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5847660005\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5908437144\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.78\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5849418248\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5910166098\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.79\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5851169495\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5911888034\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.8\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5852913784\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5913602991\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.81\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5854651156\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5915311008\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.8200000000000001\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5856381648\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5917012125\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.8300000000000001\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5858105300\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5918706381\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.84\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5859822150\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5920393814\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.85\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5861532236\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5922074465\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.86\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5863235596\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5923748369\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.87\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5864932267\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5925415567\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.88\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5866622288\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5927076095\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.89\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5868305695\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5928729991\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.9\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5869982525\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5930377293\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.91\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5871652814\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5932018037\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.92\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5873316600\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5933652260\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.93\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5874973919\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5935279999\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.9400000000000001\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5876624805\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5936901290\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.9500000000000001\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5878269296\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5938516170\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.96\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5879907426\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5940124674\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.97\n",
      "final training loss =  0.7360000000\n",
      "final training accuracy =  0.5881539231\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5941726837\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.98\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5883164746\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5943322696\n",
      "learning_rate =  1e-05\n",
      "alpha =  0.99\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5884784005\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5944912285\n",
      "learning_rate =  1e-05\n",
      "alpha =  1.0\n",
      "final training loss =  0.7380000000\n",
      "final training accuracy =  0.5886397044\n",
      "final testing loss =  0.7260000000\n",
      "final testing accuracy =  0.5946495639\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ex_learning_rate)):\n",
    "    for j in range(len(ex_alpha)):\n",
    "\n",
    "        theta                       = np.zeros(number_feature)\n",
    "\n",
    "        for k in range(number_iteration):\n",
    "            theta           = theta - ex_learning_rate[i]*compute_gradient(theta, compute_feature(data_train_point), data_train_label, ex_alpha[j])\n",
    "            #loss_train      = compute_loss(theta, compute_feature(data_train_point), data_train_label, alpha)\n",
    "            #loss_test       = compute_loss(theta, compute_feature(data_test_point), data_test_label, alpha)\n",
    "            #accuracy_train  = compute_accuracy(theta, compute_feature(data_train_point), data_train_label)\n",
    "            #accuracy_test   = compute_accuracy(theta, compute_feature(data_test_point), data_test_label)\n",
    "\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "            #loss_iteration_train[i]     = loss_train\n",
    "            #loss_iteration_test[i]      = loss_test\n",
    "            #accuracy_iteration_train[i] = accuracy_train\n",
    "            #accuracy_iteration_test[i]  = accuracy_test\n",
    "\n",
    "        loss_train      = compute_loss(theta, compute_feature(data_train_point), data_train_label, ex_alpha[j])\n",
    "        loss_test       = compute_loss(theta, compute_feature(data_test_point), data_test_label, ex_alpha[j])\n",
    "        accuracy_train  = compute_accuracy(theta, compute_feature(data_train_point), data_train_label)\n",
    "        accuracy_test   = compute_accuracy(theta, compute_feature(data_test_point), data_test_label)\n",
    "        final_loss_train.append(loss_train)\n",
    "        final_loss_test.append(loss_test)\n",
    "        final_accuracy_train.append(accuracy_train)\n",
    "        final_accuracy_test.append(accuracy_test)\n",
    "        print('learning_rate = ', ex_learning_rate[i])\n",
    "        print('alpha = ', ex_alpha[j])\n",
    "        print(\"final training loss = {:13.10f}\".format(accuracy_train))\n",
    "        print(\"final training accuracy = {:13.10f}\".format(loss_train))\n",
    "        print(\"final testing loss = {:13.10f}\".format(accuracy_test))\n",
    "        print(\"final testing accuracy = {:13.10f}\".format(loss_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1818"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for presenting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_01():\n",
    "\n",
    "    print(\"final training accuracy = {:13.10f}\".format(accuracy_iteration_train[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_02():\n",
    "\n",
    "    print(\"final testing accuracy = {:13.10f}\".format(accuracy_iteration_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_03():\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('training loss')\n",
    "\n",
    "    plt.plot(loss_iteration_train, '-', color='red')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_04():\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('testing loss')\n",
    "\n",
    "    plt.plot(loss_iteration_test, '-', color='red')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_05():\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('training accuracy')\n",
    "\n",
    "    plt.plot(accuracy_iteration_train, '-', color='red')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_06():\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('testing accuracy')\n",
    "\n",
    "    plt.plot(accuracy_iteration_test, '-', color='red')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the linear regression values over the 2-dimensional Euclidean space and superimpose the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_07():\n",
    "\n",
    "    plt.figure(figsize=(8,8)) \n",
    "    plt.title('linear regression values on the training data')\n",
    "    \n",
    "    min_x   = np.min(data_train_point_x)\n",
    "    max_x   = np.max(data_train_point_x)\n",
    "    min_y   = np.min(data_train_point_y)\n",
    "    max_y   = np.max(data_train_point_y)\n",
    "\n",
    "    X = np.arange(min_x - 0.5, max_x + 0.5, 0.1) \n",
    "    Y = np.arange(min_y - 0.5, max_y + 0.5, 0.1) \n",
    "\n",
    "    [XX, YY] = np.meshgrid(X, Y)\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    plt.plot(data_train_point_x_class_0, data_train_point_y_class_0, '.', color='blue', label='class = 0')\n",
    "    plt.plot(data_train_point_x_class_1, data_train_point_y_class_1, '.', color='red', label='class = 1')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_08():\n",
    "    \n",
    "    plt.figure(figsize=(8,8)) \n",
    "    plt.title('linear regression values on the testing data')\n",
    "    \n",
    "    min_x   = np.min(data_test_point_x)\n",
    "    max_x   = np.max(data_test_point_x)\n",
    "    min_y   = np.min(data_test_point_y)\n",
    "    max_y   = np.max(data_test_point_y)\n",
    "\n",
    "    X = np.arange(min_x - 0.5, max_x + 0.5, 0.1)\n",
    "    Y = np.arange(min_y - 0.5, max_y + 0.5, 0.1) \n",
    "\n",
    "    [XX, YY] = np.meshgrid(X, Y)\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    plt.plot(data_test_point_x_class_0, data_test_point_y_class_0, '.', color='blue', label='class = 0')\n",
    "    plt.plot(data_test_point_x_class_1, data_test_point_y_class_1, '.', color='red', label='class = 1')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the logistic regression values over the 2-dimensional Euclidean space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_09():\n",
    "\n",
    "    plt.figure(figsize=(8,8)) \n",
    "    plt.title('logistic regression values on the training data')\n",
    "    \n",
    "    min_x   = np.min(data_train_point_x)\n",
    "    max_x   = np.max(data_train_point_x)\n",
    "    min_y   = np.min(data_train_point_y)\n",
    "    max_y   = np.max(data_train_point_y)\n",
    "\n",
    "    X = np.arange(min_x - 0.5, max_x + 0.5, 0.1) \n",
    "    Y = np.arange(min_y - 0.5, max_y + 0.5, 0.1) \n",
    "\n",
    "    [XX, YY] = np.meshgrid(X, Y)\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "    plt.plot(data_train_point_x_class_0, data_train_point_y_class_0, '.', color='blue', label='class = 0')\n",
    "    plt.plot(data_train_point_x_class_1, data_train_point_y_class_1, '.', color='red', label='class = 1')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_result_10():\n",
    "    \n",
    "    plt.figure(figsize=(8,8)) \n",
    "    plt.title('logistic regression values on the testing data')\n",
    "\n",
    "    min_x   = np.min(data_test_point_x)\n",
    "    max_x   = np.max(data_test_point_x)\n",
    "    min_y   = np.min(data_test_point_y)\n",
    "    max_y   = np.max(data_test_point_y)\n",
    "\n",
    "    X = np.arange(min_x - 0.5, max_x + 0.5, 0.1) \n",
    "    Y = np.arange(min_y - 0.5, max_y + 0.5, 0.1) \n",
    "\n",
    "    [XX, YY] = np.meshgrid(X, Y)\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # complete the blanks\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "    plt.plot(data_test_point_x_class_0, data_test_point_y_class_0, '.', color='blue', label='class = 0')\n",
    "    plt.plot(data_test_point_x_class_1, data_test_point_y_class_1, '.', color='red', label='class = 1')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "## [RESULT 01]\n",
      "**************************************************\n",
      "final training accuracy =  0.8960000000\n",
      "**************************************************\n",
      "## [RESULT 02]\n",
      "**************************************************\n",
      "final testing accuracy =  0.8780000000\n",
      "**************************************************\n",
      "## [RESULT 03]\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGoCAYAAABL+58oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dfZhdZX3v//d3JuQJEkhMUAgJQUhEQAQcUaoiVnnQUrDVVmqtWFFqK/WcWqvw0z7RHqvYnx49pbUci0c9KCJaTSkVoSpoFcyEJ01iJAQwITxEAiE8BZJ8zx/3GrMzmSRrktmzZ+28X9e1rrXXvdba852dTfjkvu+1VmQmkiRJ3aSn0wVIkiSNNAOOJEnqOgYcSZLUdQw4kiSp6xhwJElS1zHgSJKkrmPAkdQREfHpiPjzkT52mDXMjYiMiHEj/d6SOiu8D46k4YqIu4F3ZOZ1na5ld0TEXOAuYK/M3NjZaiSNJHtwJI04e0QkdZoBR9KwRMQXgDnAv0XEYxHx/pahnnMi4ufAt6tjvxIR90fEuoi4ISKObHmf/xMRf1u9PikiVkXEn0bEgxFxX0T8/i4e+6yI+LeIeDQiFkbE30bE92v+bgdGxIKIWBsRyyPinS37jo+I/up9H4iIj1ftEyPi/0bEQxHxSPUzn71bH7Kk3WbAkTQsmfl7wM+BX8/MfTLzopbdrwSeD5xabf8HMA/YH7gZuGwHb/0cYF9gFnAOcHFETNuFYy8GHq+OObta6voSsAo4EHgj8OGIeHW175PAJzNzKnAocEXVfnZVy2zgWcC7gCeH8TMltYEBR9JI+qvMfDwznwTIzEszc31mbgD+CnhhROy7nXOfAS7MzGcy82rgMeB5wzk2InqBNwB/mZlPZOYS4HN1Co+I2cDLgQ9k5lOZeSvwGeD3Wn7mYRExIzMfy8wbW9qfBRyWmZsyc1FmPlrnZ0pqHwOOpJG0cuBFRPRGxEci4s6IeBS4u9o1YzvnPjRoou8TwD7DPHYmMK61jkGvd+RAYG1mrm9pu4fSSwSlp2g+8NNqGOr0qv0LwDXA5RGxOiIuioi9av5MSW1iwJG0K7Z3+WVr+5uBM4HXUIZw5lbt0b6yWANsBA5qaZtd89zVwPSImNLSNge4FyAz78jM36EMt30UuDIi9q56kf46M48AfgU4HXjrbv4eknaTAUfSrngAeO5OjpkCbAAeAiYDH253UZm5Cfga8FcRMTkiDqdm2MjMlcAPgL+rJg4fTem1uQwgIt4SETMzczPwSHXapoh4VUS8oBoee5QyZLVpZH8zScNlwJG0K/4O+FB11dD7tnPM5ylDPPcCS4Abt3PcSDuP0mN0P2X46EuUoFXH71B6mlYD/0qZy3Ntte80YHFEPEaZcHxWZj5Fmcx8JSXcLAWuB/7viPwmknaZN/qT1NUi4qPAczJzOFdTSWo4e3AkdZWIODwijo7ieMow0792ui5Jo8u7jUrqNlMow1IHAg8C/z/wjY5WJGnUOUQlSZK6jkNUkiSp63TNENWMGTNy7ty5nS5DkiSNokWLFv0iM2cObu+agDN37lz6+/s7XYYkSRpFEXHPUO0OUUmSpK5jwJEkSV3HgCNJkrqOAUeSJHWdtgaciDgtIpZFxPKIOH87x/x2RCyJiMUR8cWW9rMj4o5q8RbrkiSptrZdRVU9Wfdi4GRgFbAwIhZk5pKWY+YBFwAvy8yHI2L/qn068JdAH5DAourch9tVryRJ6h7t7ME5HliemSsy82ngcuDMQce8E7h4ILhk5oNV+6nAtZm5ttp3LeVJvpIkSTvVzoAzC1jZsr2qams1H5gfEf8VETdGxGnDOJeIODci+iOif82aNSNYuiRJarJ2BpwYom3wg6/GAfOAk4DfAT4TEfvVPJfMvCQz+zKzb+bMbW5iKEmS9lDtDDirgNkt2wcBq4c45huZ+Uxm3gUsowSeOudKkiQNqZ0BZyEwLyIOiYjxwFnAgkHHfB14FUBEzKAMWa0ArgFOiYhpETENOKVqkyRJ2qm2XUWVmRsj4jxKMOkFLs3MxRFxIdCfmQvYEmSWAJuAP8vMhwAi4m8oIQngwsxc265aJUlSd4nMbaa2NFJfX1/6sE1JkvYsEbEoM/sGt3snY0mS1HUMODuzbh0ccQR84QudrkSSJNVkwNmZTFi6FB56qNOVSJKkmgw4O9NTfUSbN3e2DkmSVJsBZ2cMOJIkNY4BZ2eiuqmyAUeSpMYw4OyMPTiSJDWOAWdnBgJOl9wvSJKkPYEBZ2fswZEkqXEMODtjwJEkqXEMODtjwJEkqXEMODvjVVSSJDWOAaeOnh4DjiRJDWLAqSPCgCNJUoMYcOro6fEycUmSGsSAU4dDVJIkNYoBpw4DjiRJjWLAqcOAI0lSoxhw6jDgSJLUKAacOgw4kiQ1igGnDi8TlySpUQw4dXiZuCRJjWLAqcMhKkmSGsWAU4cBR5KkRjHg1GHAkSSpUQw4dRhwJElqFANOHQYcSZIaxYBTh5eJS5LUKAacOrxMXJKkRjHg1OEQlSRJjWLAqcOAI0lSoxhw6jDgSJLUKAacOgw4kiQ1igGnDgOOJEmNYsCpw8vEJUlqFANOHV4mLklSoxhw6nCISpKkRmlrwImI0yJiWUQsj4jzh9j/tohYExG3Vss7WvZtamlf0M46d8qAI0lSo4xr1xtHRC9wMXAysApYGBELMnPJoEO/nJnnDfEWT2bmMe2qb1gMOJIkNUo7e3COB5Zn5orMfBq4HDizjT+vfQw4kiQ1SjsDzixgZcv2qqptsDdExO0RcWVEzG5pnxgR/RFxY0S8vo117pwBR5KkRmlnwIkh2gZfivRvwNzMPBq4Dvhcy745mdkHvBn4nxFx6DY/IOLcKgT1r1mzZqTq3paXiUuS1CjtDDirgNYemYOA1a0HZOZDmbmh2vzfwIta9q2u1iuA7wLHDv4BmXlJZvZlZt/MmTNHtvpWXiYuSVKjtDPgLATmRcQhETEeOAvY6mqoiDigZfMMYGnVPi0iJlSvZwAvAwZPTh49DlFJktQobbuKKjM3RsR5wDVAL3BpZi6OiAuB/sxcALwnIs4ANgJrgbdVpz8f+OeI2EwJYR8Z4uqr0WPAkSSpUdoWcAAy82rg6kFtf9Hy+gLggiHO+wHwgnbWNiwGHEmSGsU7GddhwJEkqVEMOHUYcCRJahQDTh1eJi5JUqMYcOqwB0eSpEYx4NThfXAkSWoUA04d9uBIktQoBpw6DDiSJDWKAacOA44kSY1iwKnDgCNJUqMYcOow4EiS1CgGnDq8D44kSY1iwKnDy8QlSWoUA04dDlFJktQoBpw6enpg06ZOVyFJkmoy4NTR22vAkSSpQQw4dYwbZ8CRJKlBDDh19PbCxo2drkKSJNVkwKnDISpJkhrFgFOHAUeSpEYx4NRhwJEkqVEMOHWMG+ccHEmSGsSAU4c9OJIkNYoBpw4DjiRJjWLAqcOAI0lSoxhw6vBGf5IkNYoBp47e3vI0cR+4KUlSIxhw6ujtLWt7cSRJagQDTh0GHEmSGsWAU8e4cWVtwJEkqREMOHUM9OB4sz9JkhrBgFOHQ1SSJDWKAacOA44kSY1iwKnDgCNJUqMYcOoYmGTsHBxJkhrBgFOHPTiSJDWKAacOA44kSY1iwKnDgCNJUqMYcOrwRn+SJDVKWwNORJwWEcsiYnlEnD/E/rdFxJqIuLVa3tGy7+yIuKNazm5nnTvljf4kSWqUce1644joBS4GTgZWAQsjYkFmLhl06Jcz87xB504H/hLoAxJYVJ37cLvq3SGHqCRJapR29uAcDyzPzBWZ+TRwOXBmzXNPBa7NzLVVqLkWOK1Nde6cAUeSpEZpZ8CZBaxs2V5VtQ32hoi4PSKujIjZwzk3Is6NiP6I6F+zZs1I1b0t5+BIktQo7Qw4MURbDtr+N2BuZh4NXAd8bhjnkpmXZGZfZvbNnDlzt4rdIefgSJLUKO0MOKuA2S3bBwGrWw/IzIcyc0O1+b+BF9U9d1Q5RCVJUqO0M+AsBOZFxCERMR44C1jQekBEHNCyeQawtHp9DXBKREyLiGnAKVVbZxhwJElqlLZdRZWZGyPiPEow6QUuzczFEXEh0J+ZC4D3RMQZwEZgLfC26ty1EfE3lJAEcGFmrm1XrTtlwJEkqVHaFnAAMvNq4OpBbX/R8voC4ILtnHspcGk766vNh21KktQo3sm4DntwJElqFANOHQYcSZIaxYBThwFHkqRGMeDUsddeZf3MM52tQ5Ik1WLAqcOAI0lSoxhw6hg/vqwNOJIkNYIBp46BHpynn+5sHZIkqRYDTh0OUUmS1CgGnDoGhqjswZEkqREMOHXYgyNJUqMYcOpwkrEkSY1iwKnDScaSJDWKAaeOnp6y2IMjSVIjGHDqGj/eHhxJkhrCgFPXXnvZgyNJUkMYcOoaP96AI0lSQxhw6tprL4eoJElqCANOXQ5RSZLUGAacupxkLElSYxhw6rIHR5KkxjDg1OUkY0mSGsOAU5eTjCVJagwDTl0OUUmS1BgGnLqcZCxJUmMYcOqyB0eSpMYw4NTlJGNJkhrDgFOXk4wlSWoMA05d48fDhg2drkKSJNVgwKlr0iQDjiRJDWHAqWviRHjyyU5XIUmSajDg1DVpkgFHkqSGMODUNXEiPPVUp6uQJEk1GHDqGpiDs3lzpyuRJEk7YcCpa+LEsnaisSRJY54Bp65Jk8raeTiSJI15Bpy6BnpwnIcjSdKYZ8Cpyx4cSZIao60BJyJOi4hlEbE8Is7fwXFvjIiMiL5qe25EPBkRt1bLp9tZZy324EiS1Bjj2vXGEdELXAycDKwCFkbEgsxcMui4KcB7gJsGvcWdmXlMu+obNntwJElqjHb24BwPLM/MFZn5NHA5cOYQx/0NcBEwtrtG7MGRJKkx2hlwZgErW7ZXVW2/FBHHArMz86ohzj8kIm6JiOsj4hVD/YCIODci+iOif82aNSNW+JDswZEkqTHaGXBiiLb85c6IHuATwJ8Ocdx9wJzMPBZ4L/DFiJi6zZtlXpKZfZnZN3PmzBEqezvswZEkqTHaGXBWAbNbtg8CVrdsTwGOAr4bEXcDLwUWRERfZm7IzIcAMnMRcCcwv4217pw9OJIkNUY7A85CYF5EHBIR44GzgAUDOzNzXWbOyMy5mTkXuBE4IzP7I2JmNUmZiHguMA9Y0cZad84eHEmSGqNtV1Fl5saIOA+4BugFLs3MxRFxIdCfmQt2cPqJwIURsRHYBLwrM9e2q9ZaJk8u68cf72gZkiRp59oWcAAy82rg6kFtf7GdY09qef1V4KvtrG3Ypkwp68ce62wdkiRpp7yTcV0DPTjr13e2DkmStFMGnLp6emCffezBkSSpAQw4w7HPPvbgSJLUAAac4ZgyxR4cSZIawIAzHFOm2IMjSVIDGHCGwzk4kiQ1ggFnOOzBkSSpEQw4w2EPjiRJjWDAGQ57cCRJagQDznB4FZUkSY1gwBmOgSGqzE5XIkmSdsCAMxxTp5ZwYy+OJEljmgFnOKZNK+uHH+5sHZIkaYcMOMMxEHDWru1sHZIkaYcMOMMxfXpZ24MjSdKYZsAZjoGAYw+OJEljmgFnOJyDI0lSIxhwhsMeHEmSGsGAMxyTJ8NeexlwJEka4ww4wxFRenEcopIkaUwz4AzXtGn24EiSNMYZcIZr+nQDjiRJY1ytgBMR/y0ipkbxLxFxc0Sc0u7ixiQDjiRJY17dHpy3Z+ajwCnATOD3gY+0raqxbOZMWLOm01VIkqQdqBtwolq/DvhsZt7W0rZnefaz4YEHYPPmTlciSZK2o27AWRQR36IEnGsiYgqwZ/4f/jnPgY0bvZJKkqQxbFzN484BjgFWZOYTETGdMky153n2s8v6/vvhWc/qbC2SJGlIdXtwTgCWZeYjEfEW4EPAuvaVNYY95zll/cADna1DkiRtV92A80/AExHxQuD9wD3A59tW1Vg20INjwJEkacyqG3A2ZmYCZwKfzMxPAlPaV9YYNtCDc//9na1DkiRtV905OOsj4gLg94BXREQvsFf7yhrD9tsPxo+3B0eSpDGsbg/Om4ANlPvh3A/MAj7WtqrGsogyTGUPjiRJY1atgFOFmsuAfSPidOCpzNwz5+CAAUeSpDGu7qMafhv4EfBbwG8DN0XEG9tZ2Jh24IGwenWnq5AkSdtRdw7OB4EXZ+aDABExE7gOuLJdhY1pc+bA9dd3ugpJkrQddefg9AyEm8pDwzi3+8yeDevWwaOPdroSSZI0hLo9ON+MiGuAL1XbbwKubk9JDTBnTlmvXAlHHtnZWiRJ0jbqTjL+M+AS4GjghcAlmfmBnZ0XEadFxLKIWB4R5+/guDdGREZEX0vbBdV5yyLi1Dp1jprZs8t65crO1iFJkoZUtweHzPwq8NW6x1f3yrkYOBlYBSyMiAWZuWTQcVOA9wA3tbQdAZwFHAkcCFwXEfMzc1Pdn99WAz04P/95Z+uQJElD2mEPTkSsj4hHh1jWR8TOJqAcDyzPzBWZ+TRwOeVOyIP9DXAR8FRL25nA5Zm5ITPvApZX7zc2HHAA9PTYgyNJ0hi1w4CTmVMyc+oQy5TMnLqT954FtCaAVVXbL0XEscDszLxquOd21LhxMGuWPTiSJI1R7bwSKoZoy1/ujOgBPgH86XDPbXmPcyOiPyL616xZs8uF7pI5cww4kiSNUe0MOKuA2S3bBwGtd8ebAhwFfDci7gZeCiyoJhrv7FwAMvOSzOzLzL6ZM2eOcPk7cfDBcPfdo/szJUlSLe0MOAuBeRFxSESMp0waXjCwMzPXZeaMzJybmXOBG4EzMrO/Ou6siJgQEYcA8yh3Uh47Djus9OA8/XSnK5EkSYO0LeBk5kbgPOAaYClwRWYujogLI+KMnZy7GLgCWAJ8E3j3mLmCasBhh8HmzfbiSJI0BtW+THxXZObVDLohYGb+xXaOPWnQ9v8A/kfbittdhx1W1nfcAfPnd7YWSZK0lT33cQu7ayDgLF/e2TokSdI2DDi7asYMmDrVgCNJ0hhkwNlVEaUXx4AjSdKYY8DZHQYcSZLGJAPO7pg3D+66C555ptOVSJKkFgac3XH44bBpk704kiSNMQac3XHkkWX9k590tg5JkrQVA87uOPzw8lTxxYs7XYkkSWphwNkdkybBoYfagyNJ0hhjwNldRx1lD44kSWOMAWd3HXlkeVzDhg2drkSSJFUMOLvrqKPKlVTLlnW6EkmSVDHg7K6jjirr22/vbB2SJOmXDDi763nPK5ONb76505VIkqSKAWd3jRsHxxwD/f2drkSSJFUMOCOhr6/04Gza1OlKJEkSBpyR8aIXweOPw89+1ulKJEkSBpyR0ddX1g5TSZI0JhhwRsLhh8PkybBoUacrkSRJGHBGRm8vHHcc3HRTpyuRJEkYcEbOy15WhqieeKLTlUiStMcz4IyUV7wCNm6EH/2o05VIkrTHM+CMlJe9DCLge9/rdCWSJO3xDDgjZb/94AUvMOBIkjQGGHBG0stfDj/8YRmqkiRJHWPAGUmveAU89hjcckunK5EkaY9mwBlJv/qrZf2tb3W2DkmS9nAGnJG0//7lfjjXXNPpSiRJ2qMZcEbaqafCD34A69Z1uhJJkvZYBpyRduqp5ani3/52pyuRJGmPZcAZaSecAFOmOEwlSVIHGXBG2vjx8JrXwFVXwebNna5GkqQ9kgGnHX7zN+Hee31sgyRJHWLAaYfTT4dx4+BrX+t0JZIk7ZEMOO2w337w6leXgJPZ6WokSdrjGHDa5Q1vgDvvhNtv73QlkiTtcQw47XLmmdDbC1/6UqcrkSRpj2PAaZf994fTToMvfKHcF0eSJI2atgaciDgtIpZFxPKIOH+I/e+KiB9HxK0R8f2IOKJqnxsRT1btt0bEp9tZZ9ucfTasXg3/+Z+drkSSpD1K2wJORPQCFwOvBY4AfmcgwLT4Yma+IDOPAS4CPt6y787MPKZa3tWuOtvq13+9TDj+/Oc7XYkkSXuUdvbgHA8sz8wVmfk0cDlwZusBmfloy+beQHddcjRxIpx1VrmaymdTSZI0atoZcGYBK1u2V1VtW4mId0fEnZQenPe07DokIm6JiOsj4hVD/YCIODci+iOif82aNSNZ+8g55xx48kl7cSRJGkXtDDgxRNs2PTSZeXFmHgp8APhQ1XwfMCczjwXeC3wxIqYOce4lmdmXmX0zZ84cwdJHUF8fvOQl8A//4KMbJEkaJe0MOKuA2S3bBwGrd3D85cDrATJzQ2Y+VL1eBNwJzG9Tne133nnws5/Bddd1uhJJkvYI7Qw4C4F5EXFIRIwHzgIWtB4QEfNaNn8NuKNqn1lNUiYingvMA1a0sdb2+q3fKpeN/6//1elKJEnaI7Qt4GTmRuA84BpgKXBFZi6OiAsj4ozqsPMiYnFE3EoZijq7aj8RuD0ibgOuBN6VmWvbVWvbTZgAf/AH8O//DkuXdroaSZK6XmSXPCupr68v+/v7O13G9q1ZA3PnwhvfCJ/7XKerkSSpK0TEoszsG9zunYxHy8yZcO65cNllsKK5o22SJDWBAWc0ve995flUF13U6UokSepqBpzRNGsWvP3t8NnPwt13d7oaSZK6lgFntH3wg9DTA3/+552uRJKkrmXAGW0HHQT//b+XuTi33trpaiRJ6koGnE74wAdg2rSyliRJI86A0wn77Qcf+hB861vl3jiSJGlEGXA65d3vhiOOgD/+4/IwTkmSNGIMOJ0yfjxcfDHcdRf83d91uhpJkrqKAaeTTjoJ3vIW+OhHYdmyTlcjSVLXMOB02sc+BpMnwznnwKZNna5GkqSuYMDptOc8pzxl/L/+Cz7xiU5XI0lSVzDgjAW/+7vwG79RbgK4eHGnq5EkqfEMOGNBBHz60zB1Krz1rbBhQ6crkiSp0Qw4Y8X++8NnPgM33wzvf3+nq5EkqdEMOGPJmWeWxzh86lNw5ZWdrkaSpMYy4Iw1H/0ovOQl5anjy5d3uhpJkhrJgDPWjB8PV1wBe+1VJh6vX9/piiRJahwDzlg0Zw5cfjksXQpvfrP3x5EkaZgMOGPVySeXuThXXQXnn9/paiRJapRxnS5AO/BHf1R6cf7+72H+fHjnOztdkSRJjWDAGes+8Qm4805417tgxowyL0eSJO2QQ1Rj3bhx8JWvlCurzjoLvv3tTlckSdKYZ8Bpgr33LnNx5s8v98pZuLDTFUmSNKYZcJpi+nS45poyTHXKKdDf3+mKJEkasww4TXLggfCd78C0afCa18BNN3W6IkmSxiQDTtPMnQvf/S4861nlUvIf/rDTFUmSNOYYcJpozhy4/np49rNLyLnmmk5XJEnSmGLAaaqDDoIbboDDDoPTT4cvfKHTFUmSNGYYcJrsgANKT86JJ8Jb3woXXQSZna5KkqSOM+A03b77wtVXw5veBB/4QLn78TPPdLoqSZI6yjsZd4MJE+CLX4SDDy69OEuXlpsDzpzZ6cokSeoIe3C6RU8PfPSjZS7OjTfCi18Mt9/e6aokSeoIA063ectb4HvfK8NUJ5wAl13W6YokSRp1Bpxu9OIXlzsdH3dcCTzveAc88USnq5IkadQYcLrVAQeUux5fcAH8y7+Uh3UuXdrpqiRJGhUGnG42bhx8+MPwzW/CAw9AXx98+tNeSi5J6nptDTgRcVpELIuI5RFx/hD73xURP46IWyPi+xFxRMu+C6rzlkXEqe2ss+udeirceiv8yq/AH/4hnHYarFrV6aokSWqbtgWciOgFLgZeCxwB/E5rgKl8MTNfkJnHABcBH6/OPQI4CzgSOA34x+r9tKsOPLA80uHii+H734ejjipXXNmbI0nqQu3swTkeWJ6ZKzLzaeBy4MzWAzLz0ZbNvYGB/9ueCVyemRsy8y5gefV+2h09PeVGgLfdVgLOW98KZ5wB99zT6cokSRpR7Qw4s4CVLdurqratRMS7I+JOSg/Oe4Z57rkR0R8R/WvWrBmxwrveYYeVRzz8/d/Dt78NRxxRXnsHZElSl2hnwIkh2rYZD8nMizPzUOADwIeGee4lmdmXmX0zvWvv8PT2wp/+KSxZAq9+NfzZn5VJyDfe2OnKJEnabe0MOKuA2S3bBwGrd3D85cDrd/Fc7aqDD4ZvfAO+9jV46KFyc8C3vQ1W+3FLkpqrnQFnITAvIg6JiPGUScMLWg+IiHktm78G3FG9XgCcFRETIuIQYB7wozbWumeLgN/4jXKfnA98AL70JZg3Dy680BsESpIaqW0BJzM3AucB1wBLgSsyc3FEXBgRZ1SHnRcRiyPiVuC9wNnVuYuBK4AlwDeBd2fmpnbVqsqUKfCRj5Sg87rXwV/+JTzveeVqq82bO12dJEm1RXbJZcJ9fX3Z39/f6TK6yw03wJ/8Cdx8c7nq6sIL4fWvLz0+kiSNARGxKDP7Brd7J2Nt34knwsKFcPnl8PTT8Ju/CccfX+6M3CXBWJLUnQw42rGeHnjTm2DxYvjsZ+EXv4DXvraEn2uuMehIksYkA47qGTeuXF21bBn84z/CXXeVRz709cFXvgKbnCIlSRo7DDganvHjy/Os7rwTPvMZWL8efvu3y80CL720DGVJktRhBhztmgkT4JxzyhVXV1wBe+9dtufOhb/9W3jwwU5XKEnagxlwtHt6e+G3fgsWLSqTj48+Gv78z2H27DKkdcstna5QkrQHMuBoZETAqaeWkLN0KbzznXDllXDccfCKV8CXvwwbNnS6SknSHsKAo5F3+OHwD/8Aq1bBxz8O994LZ50Fs2bBe99bnn8lSVIbGXDUPvvtV24UuHx5uaT8Va8qwefII+FlLyuXnT/+eKerlCR1IQOO2q+nB045pVxOvmoVfOxj5cGeb387HHAA/P7vw3/+p5eaS5JGjAFHo2v//eF97yvzdL73PXjjG8uTzF/zGpgzp+y79VZvIChJ2i0GHHVGBLz85eXeOfffXy417+uDT30Kjj0WXvAC+PCH4Wc/63SlkqQGMuCo8yZNKpeaf+MbcN998E//BNOmwQc/WJ5mfvTR8Nd/XR4XYc+OJKkGnyausWvlyjJ89dWvwve/X8LN854Hb3hDGdo65hifbC5Je7jtPU3cgKNmuP9++Nd/LffW+e53YfPmcjPB008vy6teVXqCJEl7FAOOujyuZxkAABCaSURBVMcvflGGs666Cq69tlxqPmlSmah8+unwa79W7rkjSep6Bhx1p6eeguuvL2Hnqqvg7rtL+zHHlKedn3xyuefOhAkdLVOS1B4GHHW/zHL5+UDY+eEPYePG0rtz4okl7Jx8crlCy7k7ktQVDDja86xfX+brXHttWX7609L+7GeX4ayTT4aTToKDD+5klZKk3WDAkVat2hJ2rrsO1qwp7XPmwCtfWXp5XvlKOOwwe3gkqSEMOFKrzZvhJz+BG24oc3huuAEefLDsO+CAEnYGAs/zn18eNyFJGnMMONKOZMKyZVvCzvXXl6egQ7np4EteAiecUJaXvASmTu1svZIkwIAjDU8m3HVXCTo/+EGZsLxkSWmPKE9EP+EEeOlLy/p5z7OXR5I6wIAj7a516+BHPyph54c/hBtvhEceKfumTYPjj4cXvag8U+tFLyo3InQujyS1lQFHGmmbN5dhrYHAs3BhmdezaVPZP3NmCTqtoeeggww9kjSCthdwxnWiGKkr9PSUCcjPfz68/e2l7ckn4fbbYdEi6O8v62uv3RJ69t9/S+h54QvLg0QPPRR6ezv3e0hSFzLgSCNp0qQyCfklL9nS9uSTcNttW4eeb31rS+iZPBmOOqoEnoHQc/TRsO++nfkdJKkLOEQldcJTT5VJy7fdVpbbby/rtWu3HDN3bgk6A6HnyCPLPXr22qtjZUvSWOMQlTSWTJwIxx1XlgGZsHr1tqHnqqvKfB+AceNg/vwSdo44Yssyb57P25KkFgYcaayIKE9BnzULXve6Le1PPlmesbVkSVkWL4ZbboErryyhCMocnsMOK2GnNfzMm1eGwCRpD2PAkca6SZO27e2BEnx+9rMSeAbCz5IlsGDBlvk9UC5Xnz+/3Ktn/vwty8EHlx4hSepC/u0mNdWkSVsmJrfasKEEnyVLynpgueyyci+fAXvtVa7gGhx85s8vDyT1cnZJDWbAkbrNhAnwgheUpVVmecBoa+gZWL75zRKMBkyZAs99blkOPXTr9cEHO9FZ0phnwJH2FBHlPjz77w8vf/nW+zZtgpUrtw49K1bAT38KV1+9dfjp6SlPYB8cfAbW++03ur+XJA3BgCOpTFKeO7csp5yy9b7Nm+G+++DOO0voaV1//eulV6jV9Olb3uvgg7ddpk1z+EtS2xlwJO1YT8+Wq7tOPHHb/Y8+Wh5M2hp87r67XPn1zW/CE09sffw++2w//Bx8cJn/44NLJe0mA46k3TN16tCTnaHM+3nooRJ47rln2+UHP4CHH976nAkTyhDYwQeXZ3cNtUyfbi+QpB1qa8CJiNOATwK9wGcy8yOD9r8XeAewEVgDvD0z76n2bQJ+XB3688w8o521SmqDCJgxoyx929xotHj00aHDzz33wHXXlZsfDtzocMCkSdsPPwPLjBn2BEl7sLYFnIjoBS4GTgZWAQsjYkFmLmk57BagLzOfiIg/BC4C3lTtezIzj2lXfZLGiKlTh77qa8DGjfDAA7BqVVlWrtzyetUquOEGuPfeclyr8ePLsNpA4DngADjwwLJufT1lir1BUhdqZw/O8cDyzFwBEBGXA2cCvww4mfmdluNvBN7SxnokNdG4cVvmALU+xLTV5s3w4INbB5/WIHTTTWWi9JNPbnvu5MlDB5/B2/vtZxCSGqSdAWcWsLJlexWwnb+dADgH+I+W7YkR0U8ZvvpIZn598AkRcS5wLsCcOXN2u2BJDdXTA895Tlm2NxSWWYbDVq8uYee++7Z9fcst8O//Do8/vu35EyduHXwOOKBMiG5d9t+/rH08htRx7Qw4Q/1TZ8hHl0fEW4A+4JUtzXMyc3VEPBf4dkT8ODPv3OrNMi8BLoHyNPGRKVtSV4qAffcty/Ofv+Nj16/ffgi6777yeIxrry2BaSj77LN14BkqBA0sU6faMyS1QTsDzipgdsv2QcDqwQdFxGuADwKvzMxf3k0sM1dX6xUR8V3gWODOwedL0oibMqUs8+fv+LinnipDYw88sGXdujz4INxxB3z/++Vqshzi32ETJmwbevbfH2bOLMuMGVuvJ082EEk1tDPgLATmRcQhwL3AWcCbWw+IiGOBfwZOy8wHW9qnAU9k5oaImAG8jDIBWZLGjokTyyXtdYbIN26EX/xi2wDUuj0wTPbgg9tOmm79mUMFn+2tp08vN3KU9jBtCziZuTEizgOuoVwmfmlmLo6IC4H+zFwAfAzYB/hKlH+RDFwO/nzgnyNiM9BDmYOzZMgfJElNMG7clnlCO5NZHoy6Zk0JRTtaL19e1uvXD/1eESXkDFyu3xp+nvWsskyfXpbW1z5vTA0XOVSXaQP19fVlf39/p8uQpM7YsKEMg9UJRb/4RVm210sEZYiuNfQMFYQGt02bVoKcNIoiYlFmbnN1gd9ESeoGEyaUS9oPPLDe8QNXla1dW4LR2rVbvx68/vnPy+uHH972xout9t13+0Fo2rRyuf1Qa+9HpBFmwJGkPVHrVWWHHFL/vM2by/BZ3WC0YkV5/cgjQ0+yHtDTU8LO4OCzo1DUuh4/fvc/E3UVA44kqb6eni3B49BD65+3aVPpMXrkkdILVGd9771btjds2PH7T5684wA0deqWQDewtLbtvbc9SF3GgCNJar/e3i3BaDg9RgOeempL8KkbjhYvLtvr1u2492igvqlTdx6EdtQ2ZYpXrI0hBhxJ0tjXeifp4dq8GR57rPQgrVu39bKjtnvvhSVLtmw/88zOf9Y++2w/BE2duuUeS1OmbLvd2jZhgj1Ku8mAI0nqbj09W3pnDjpo194js/Qi1QlGrdsPPwx3371l+4kn6v28ceOGDkE7CkXb29577/IZ7GEMOJIk7UwETJpUljr3MtqeTZtKb9L69WV59NEtr4fabm1bt648PLb1mB1d0dZa+z77bD8E7bPP8Jfx48d8D5MBR5Kk0dLbu2XIandlwpNPDj8oDSx33VUeLLt+fQlddXuXoPQwDScQ/cEflMneo8iAI0lSE0WUq8cmTy7PMNtdmzaVkPPYY7u+rF695fVAcMqE3/s9A44kSeqA3t4tw1YjZaCXaeLEkXvPmgw4kiSpPQZ6mTpgz5tWLUmSup4BR5IkdR0DjiRJ6joGHEmS1HUMOJIkqesYcCRJUtcx4EiSpK5jwJEkSV3HgCNJkrqOAUeSJHUdA44kSeo6BhxJktR1DDiSJKnrGHAkSVLXiczsdA0jIiLWAPe08UfMAH7RxvfXtvzMR5+f+ejzMx9dft6jr92f+cGZOXNwY9cEnHaLiP7M7Ot0HXsSP/PR52c++vzMR5ef9+jr1GfuEJUkSeo6BhxJktR1DDj1XdLpAvZAfuajz8989PmZjy4/79HXkc/cOTiSJKnr2IMjSZK6jgFHkiR1HQPOTkTEaRGxLCKWR8T5na6nySJidkR8JyKWRsTiiPhvVfv0iLg2Iu6o1tOq9oiIT1Wf/e0RcVzLe51dHX9HRJzdqd+pKSKiNyJuiYirqu1DIuKm6vP7ckSMr9onVNvLq/1zW97jgqp9WUSc2pnfpBkiYr+IuDIiflp930/we95eEfEn1d8rP4mIL0XERL/nIysiLo2IByPiJy1tI/a9jogXRcSPq3M+FRGxWwVnpst2FqAXuBN4LjAeuA04otN1NXUBDgCOq15PAX4GHAFcBJxftZ8PfLR6/TrgP4AAXgrcVLVPB1ZU62nV62md/v3G8gK8F/gicFW1fQVwVvX608AfVq//CPh09fos4MvV6yOq7/8E4JDqv4veTv9eY3UBPge8o3o9HtjP73lbP+9ZwF3ApGr7CuBtfs9H/HM+ETgO+ElL24h9r4EfASdU5/wH8NrdqdcenB07HliemSsy82ngcuDMDtfUWJl5X2beXL1eDyyl/MV0JuV/CFTr11evzwQ+n8WNwH4RcQBwKnBtZq7NzIeBa4HTRvFXaZSIOAj4NeAz1XYAvwpcWR0y+DMf+LO4Enh1dfyZwOWZuSEz7wKWU/770CARMZXyP4J/AcjMpzPzEfyet9s4YFJEjAMmA/fh93xEZeYNwNpBzSPyva72Tc3MH2ZJO59vea9dYsDZsVnAypbtVVWbdlPVJXwscBPw7My8D0oIAvavDtve5++fy/D8T+D9wOZq+1nAI5m5sdpu/fx++dlW+9dVx/uZ1/dcYA3w2WpY8DMRsTd+z9smM+8F/h74OSXYrAMW4fd8NIzU93pW9Xpw+y4z4OzYUON/Xle/myJiH+CrwH/PzEd3dOgQbbmDdg0SEacDD2bmotbmIQ7NnezzM69vHKUb/58y81jgcUrX/fb4me+mat7HmZRhpQOBvYHXDnGo3/PRM9zPeMQ/ewPOjq0CZrdsHwSs7lAtXSEi9qKEm8sy82tV8wNV9yTV+sGqfXufv38u9b0MOCMi7qYMsf4qpUdnv6orH7b+/H752Vb796V0SfuZ17cKWJWZN1XbV1ICj9/z9nkNcFdmrsnMZ4CvAb+C3/PRMFLf61XV68Htu8yAs2MLgXnVTPzxlMloCzpcU2NVY9z/AizNzI+37FoADMykPxv4Rkv7W6vZ+C8F1lVdoNcAp0TEtOpfbqdUbRokMy/IzIMycy7l+/vtzPxd4DvAG6vDBn/mA38Wb6yOz6r9rOrqk0OAeZQJgRokM+8HVkbE86qmVwNL8HveTj8HXhoRk6u/ZwY+c7/n7Tci3+tq3/qIeGn1Z/jWlvfaNZ2elT3WF8pM8J9RZtN/sNP1NHkBXk7pcrwduLVaXkcZ+/5P4I5qPb06PoCLq8/+x0Bfy3u9nTIBcDnw+53+3ZqwACex5Sqq51L+4l4OfAWYULVPrLaXV/uf23L+B6s/i2Xs5tUN3b4AxwD91Xf965SrRfyet/cz/2vgp8BPgC9QroTyez6yn/GXKHOcnqH0uJwzkt9roK/687sT+Aeqpy3s6uKjGiRJUtdxiEqSJHUdA44kSeo6BhxJktR1DDiSJKnrGHAkSVLXMeBIGnUR8YNqPTci3jzC7/3/DfWzJO1ZvExcUsdExEnA+zLz9GGc05uZm3aw/7HM3Gck6pPUXPbgSBp1EfFY9fIjwCsi4taI+JOI6I2Ij0XEwoi4PSL+oDr+pIj4TkR8kXLTMCLi6xGxKCIWR8S5VdtHKE+UvjUiLmv9WdUdVT8WET+JiB9HxJta3vu7EXFlRPw0Ii6r7qQqqcHG7fwQSWqb82npwamCyrrMfHFETAD+KyK+VR17PHBUZt5Vbb89M9dGxCRgYUR8NTPPj4jzMvOYIX7Wb1LuMPxCYEZ1zg3VvmOBIynPvvkvyjO8vj/yv66k0WIPjqSx5BTK82tuBW6i3AZ+XrXvRy3hBuA9EXEbcCPl4X3z2LGXA1/KzE2Z+QBwPfDilvdelZmbKY8QmTsiv42kjrEHR9JYEsAfZ+ZWD5Ws5uo8Pmj7NcAJmflERHyX8nyhnb339mxoeb0J/26UGs8eHEmdtB6Y0rJ9DfCHEbEXQETMj4i9hzhvX+DhKtwcDry0Zd8zA+cPcgPwpmqez0zgRHxStNS1/FeKpE66HdhYDTX9H+CTlOGhm6uJvmuA1w9x3jeBd0XE7ZSnPt/Ysu8S4PaIuDkzf7el/V+BE4DbKE+1f39m3l8FJEldxsvEJUlS13GISpIkdR0DjiRJ6joGHEmS1HUMOJIkqesYcCRJUtcx4EiSpK5jwJEkSV3n/wEOtC18hQ2b4gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "## [RESULT 04]\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "## [RESULT 05]\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "## [RESULT 06]\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "## [RESULT 07]\n",
      "**************************************************\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'function_result_07' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2846470133c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'**************************************************'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'function_result_07' is not defined"
     ]
    }
   ],
   "source": [
    "number_result = 10\n",
    "\n",
    "for i in range(number_result):\n",
    "    title = '## [RESULT {:02d}]'.format(i+1)\n",
    "    name_function = 'function_result_{:02d}()'.format(i+1)\n",
    "\n",
    "    print('**************************************************')\n",
    "    print(title)\n",
    "    print('**************************************************')\n",
    "    eval(name_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
